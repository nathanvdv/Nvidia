{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>difficulty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Les coûts kilométriques réels peuvent diverger...</td>\n",
       "      <td>C1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Le bleu, c'est ma couleur préférée mais je n'a...</td>\n",
       "      <td>A1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Le test de niveau en français est sur le site ...</td>\n",
       "      <td>A1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Est-ce que ton mari est aussi de Boston?</td>\n",
       "      <td>A1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Dans les écoles de commerce, dans les couloirs...</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                           sentence difficulty\n",
       "0   0  Les coûts kilométriques réels peuvent diverger...         C1\n",
       "1   1  Le bleu, c'est ma couleur préférée mais je n'a...         A1\n",
       "2   2  Le test de niveau en français est sur le site ...         A1\n",
       "3   3           Est-ce que ton mari est aussi de Boston?         A1\n",
       "4   4  Dans les écoles de commerce, dans les couloirs...         B1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"Data/training_data.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows of the dataset to understand its structure\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(id            0\n",
       " sentence      0\n",
       " difficulty    0\n",
       " dtype: int64,\n",
       "    id                                           sentence difficulty  \\\n",
       " 0   0  Les coûts kilométriques réels peuvent diverger...         C1   \n",
       " 1   1  Le bleu, c'est ma couleur préférée mais je n'a...         A1   \n",
       " 2   2  Le test de niveau en français est sur le site ...         A1   \n",
       " 3   3           Est-ce que ton mari est aussi de Boston?         A1   \n",
       " 4   4  Dans les écoles de commerce, dans les couloirs...         B1   \n",
       " \n",
       "    difficulty_encoded  \n",
       " 0                   4  \n",
       " 1                   0  \n",
       " 2                   0  \n",
       " 3                   0  \n",
       " 4                   2  )"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = data.isnull().sum()\n",
    "\n",
    "# Encode the 'difficulty' column\n",
    "label_encoder = LabelEncoder()\n",
    "data['difficulty_encoded'] = label_encoder.fit_transform(data['difficulty'])\n",
    "\n",
    "# Display missing values and the first few rows after encoding\n",
    "missing_values, data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test set loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1200 entries, 0 to 1199\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        1200 non-null   int64 \n",
      " 1   sentence  1200 non-null   object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 18.9+ KB\n"
     ]
    }
   ],
   "source": [
    "test_set = pd.read_csv(\"Data/unlabelled_test_data.csv\")\n",
    "test_set.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-07 21:42:50.829198: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-07 21:42:50.829366: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-07 21:42:50.831296: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-07 21:42:51.118673: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-07 21:42:58.093996: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-07 21:42:58.177628: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-07 21:42:58.178016: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id                                           sentence difficulty  \\\n",
      "0   0  Les coûts kilométriques réels peuvent diverger...         C1   \n",
      "1   1  Le bleu, c'est ma couleur préférée mais je n'a...         A1   \n",
      "2   2  Le test de niveau en français est sur le site ...         A1   \n",
      "3   3           Est-ce que ton mari est aussi de Boston?         A1   \n",
      "4   4  Dans les écoles de commerce, dans les couloirs...         B1   \n",
      "\n",
      "   difficulty_encoded  \n",
      "0                   4  \n",
      "1                   0  \n",
      "2                   0  \n",
      "3                   0  \n",
      "4                   2  \n"
     ]
    }
   ],
   "source": [
    "# Load the French language model\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "def clean_french_sentences(sentence):\n",
    "    if isinstance(sentence, str):\n",
    "        # Apply cleaning steps\n",
    "        sentence = re.sub(r'[^a-zA-ZéèàêâôûùçÉÈÀÊÂÔÛÙÇ\\s]', '', sentence)\n",
    "        sentence = sentence.lower()\n",
    "        doc = nlp(sentence)\n",
    "        return ' '.join([token.lemma_ for token in doc])\n",
    "    return sentence  # Return as-is if not a string\n",
    "\n",
    "data = clean_french_sentences(data)\n",
    "test_set = clean_french_sentences(test_set)\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function of feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import CamembertTokenizer, CamembertModel\n",
    "import spacy\n",
    "import pyphen\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from lexicalrichness import LexicalRichness\n",
    "import numpy as np\n",
    "import textstat\n",
    "\n",
    "# Load Spacy French model\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "# Initialize Camembert\n",
    "tokenizer = CamembertTokenizer.from_pretrained('camembert/camembert-large')\n",
    "model = CamembertModel.from_pretrained('camembert/camembert-large')\n",
    "\n",
    "# Function to calculate features for a given text\n",
    "def calculate_features(text):\n",
    "    # Tokenize the text into words and sentences\n",
    "    words = word_tokenize(text, language='french')\n",
    "    sentences = sent_tokenize(text, language='french')\n",
    "\n",
    "    # Initialize Pyphen for syllable counting\n",
    "    dic = pyphen.Pyphen(lang='fr')\n",
    "\n",
    "    # Compute text embeddings\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "    # Lexical Diversity Measures\n",
    "    lex = LexicalRichness(text)\n",
    "    mtld = lex.mtld(threshold=0.72)\n",
    "\n",
    "    # Syntactic Complexity Measures\n",
    "    doc = nlp(text)\n",
    "    num_subordinate_clauses = sum(1 for sent in doc.sents for token in sent if token.dep_ in ['csubj', 'csubjpass', 'advcl'])\n",
    "    average_verbs_per_sentence = sum(1 for token in doc if token.pos_ == 'VERB') / len(sentences)\n",
    "\n",
    "    # Readability Scores\n",
    "    dcrs = textstat.dale_chall_readability_score(text)\n",
    "    fkg = textstat.flesch_kincaid_grade(text)\n",
    "    ari = textstat.automated_readability_index(text)\n",
    "    cli = textstat.coleman_liau_index(text)\n",
    "\n",
    "    return {\n",
    "        'LEN': len(words),\n",
    "        'AWL': np.mean([len(word) for word in words]),\n",
    "        'TTR': len(set(words)) / len(words),\n",
    "        'ASL': np.mean([len(word_tokenize(sentence, language='french')) for sentence in sentences]),\n",
    "        'AVPS': average_verbs_per_sentence,\n",
    "        'ASL.AVPS': np.mean([len(word_tokenize(sentence, language='french')) for sentence in sentences]) * average_verbs_per_sentence,\n",
    "        'embeddings': embeddings.tolist(),  # Convert to list for easier handling\n",
    "        'mtld': mtld,\n",
    "        'num_subordinate_clauses': num_subordinate_clauses,\n",
    "        'DCRS': dcrs,\n",
    "        'FKG': fkg,\n",
    "        'ARI': ari,\n",
    "        'CLI': cli\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applied on train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id                                           sentence difficulty  \\\n",
      "0   0  Les coûts kilométriques réels peuvent diverger...         C1   \n",
      "1   1  Le bleu, c'est ma couleur préférée mais je n'a...         A1   \n",
      "2   2  Le test de niveau en français est sur le site ...         A1   \n",
      "3   3           Est-ce que ton mari est aussi de Boston?         A1   \n",
      "4   4  Dans les écoles de commerce, dans les couloirs...         B1   \n",
      "\n",
      "   difficulty_encoded  LEN       AWL       TTR   ASL  AVPS  ASL.AVPS  \\\n",
      "0                   4   44  4.954545  0.704545  44.0   4.0     176.0   \n",
      "1                   0   14  3.642857  1.000000  14.0   2.0      28.0   \n",
      "2                   0   14  3.857143  0.928571  14.0   1.0      14.0   \n",
      "3                   0    9  3.666667  1.000000   9.0   1.0       9.0   \n",
      "4                   2   39  4.564103  0.794872  39.0   4.0     156.0   \n",
      "\n",
      "                                          embeddings       mtld  \\\n",
      "0  [0.02159704454243183, -0.21623103320598602, -0...  44.888889   \n",
      "1  [0.16034089028835297, -0.11748447269201279, 0....  54.880000   \n",
      "2  [0.12377934902906418, -0.2293163388967514, 0.0...  27.440000   \n",
      "3  [0.24752569198608398, -0.09485947340726852, 0....   8.000000   \n",
      "4  [0.0487404391169548, -0.22702157497406006, 0.1...  28.495275   \n",
      "\n",
      "   num_subordinate_clauses   DCRS   FKG   ARI    CLI  \n",
      "0                        0  17.57  18.1  24.6  15.39  \n",
      "1                        0  18.71   0.9   4.6   4.57  \n",
      "2                        0  16.43   3.6   4.6   5.03  \n",
      "3                        0  17.85   2.9   2.0   2.86  \n",
      "4                        0  16.93  14.2  20.0  11.79  \n"
     ]
    }
   ],
   "source": [
    "features_df = pd.DataFrame(data['sentence'].apply(calculate_features).tolist())\n",
    "df = data.join(features_df)\n",
    "\n",
    "# Example of accessing features of the first text\n",
    "print(df.head())\n",
    "# Save to csv file\n",
    "df.to_csv('Data/Cleaned_Enhanced_training.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applied on the testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id                                           sentence  LEN       AWL  \\\n",
      "0   0  Nous dûmes nous excuser des propos que nous eû...   10  5.000000   \n",
      "1   1  Vous ne pouvez pas savoir le plaisir que j'ai ...   15  4.400000   \n",
      "2   2  Et, paradoxalement, boire froid n'est pas la b...   12  4.166667   \n",
      "3   3  Ce n'est pas étonnant, car c'est une saison my...   10  4.700000   \n",
      "4   4  Le corps de Golo lui-même, d'une essence aussi...   78  4.987179   \n",
      "\n",
      "        TTR   ASL  AVPS  ASL.AVPS  \\\n",
      "0  0.900000  10.0   3.0      30.0   \n",
      "1  1.000000  15.0   4.0      60.0   \n",
      "2  0.916667  12.0   1.0      12.0   \n",
      "3  1.000000  10.0   0.0       0.0   \n",
      "4  0.756410  78.0   8.0     624.0   \n",
      "\n",
      "                                          embeddings       mtld  \\\n",
      "0  [-0.07058697938919067, -0.17462338507175446, -...  12.000000   \n",
      "1  [0.06289323419332504, -0.09950374811887741, 0....  15.000000   \n",
      "2  [0.1979207843542099, 0.25786763429641724, -0.0...  10.000000   \n",
      "3  [-0.002974431961774826, -0.009397363290190697,...  33.880000   \n",
      "4  [0.1390535682439804, -0.11944965273141861, 0.0...  95.134118   \n",
      "\n",
      "   num_subordinate_clauses   DCRS   FKG   ARI    CLI  \n",
      "0                        0  16.76   4.8   7.1  10.24  \n",
      "1                        0  20.12   4.0   7.8   8.63  \n",
      "2                        0  18.12   3.3   9.3  10.58  \n",
      "3                        0  18.12   3.3   7.7   9.31  \n",
      "4                        0  19.27  30.2  40.0  14.30  \n"
     ]
    }
   ],
   "source": [
    "test_set_df = pd.DataFrame(test_set['sentence'].apply(calculate_features).tolist())\n",
    "test_df = test_set.join(test_set_df)\n",
    "\n",
    "# Example of accessing features of the first text\n",
    "print(test_df.head())\n",
    "# Save to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv('Data/Cleaned_Enhanced_test.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
