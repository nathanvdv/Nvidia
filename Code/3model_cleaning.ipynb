{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path_training = \"Data/training_data.csv\"\n",
    "file_path_test = \"Data/unlabelled_test_data.csv\"\n",
    "training_data = pd.read_csv(file_path_training)\n",
    "test_data = pd.read_csv(file_path_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few rows of the DataFrame:\n",
      "    id                                           sentence difficulty\n",
      "0    0  Les coûts kilométriques réels peuvent diverger...         C1\n",
      "1    1  Le bleu, c'est ma couleur préférée mais je n'a...         A1\n",
      "2    2  Le test de niveau en français est sur le site ...         A1\n",
      "3    3           Est-ce que ton mari est aussi de Boston?         A1\n",
      "4    4  Dans les écoles de commerce, dans les couloirs...         B1\n",
      "5    5  voilà une autre histoire que j'ai beaucoup aimée.         A2\n",
      "6    6  Les médecins disent souvent qu'on doit boire u...         A2\n",
      "7    7  Il est particulièrement observé chez les perso...         B2\n",
      "8    8  J'ai retrouvé le plaisir de manger un oeuf à l...         A2\n",
      "9    9  Nous allons bien, nous habitons dans une petit...         B1\n",
      "10  10                            Bonjour et bonne année.         A1\n",
      "11  11  La presse s'est abondamment fait l'écho des ef...         B2\n",
      "12  12  Pour que le rocher s'ouvre, il faut le toucher...         B1\n",
      "13  13  J'habite une belle ville dans le nord de la Fr...         A1\n",
      "14  14  Certes il doit répondre aux goûts du consommat...         B2\n",
      "15  15  Ma timidité me quittait dès que je m'éloignais...         C2\n",
      "16  16  Je pense que fréquenter les galeries est la me...         B2\n",
      "17  17  Le soir, je me cuche tôt parce que je dois êtr...         A1\n",
      "18  18  La mère de Raphaël et de Vanessa exprime sa sa...         B2\n",
      "19  19            Je ne fais pas grand-chose à la maison.         A2\n",
      "   id                                           sentence\n",
      "0   0  Nous dûmes nous excuser des propos que nous eû...\n",
      "1   1  Vous ne pouvez pas savoir le plaisir que j'ai ...\n",
      "2   2  Et, paradoxalement, boire froid n'est pas la b...\n",
      "3   3  Ce n'est pas étonnant, car c'est une saison my...\n",
      "4   4  Le corps de Golo lui-même, d'une essence aussi...\n"
     ]
    }
   ],
   "source": [
    "# Display the first few rows of the DataFrame\n",
    "print(\"First few rows of the DataFrame:\")\n",
    "print(training_data.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id                                           sentence difficulty  \\\n",
      "0   0  Les coûts kilométriques réels peuvent diverger...         C1   \n",
      "1   1  Le bleu, c'est ma couleur préférée mais je n'a...         A1   \n",
      "2   2  Le test de niveau en français est sur le site ...         A1   \n",
      "3   3           Est-ce que ton mari est aussi de Boston?         A1   \n",
      "4   4  Dans les écoles de commerce, dans les couloirs...         B1   \n",
      "\n",
      "                                  processed_sentence  \n",
      "0  les coûts kilométriques réels peuvent diverger...  \n",
      "1  le bleu cest ma couleur préférée mais je naime...  \n",
      "2  le test de niveau en français est sur le site ...  \n",
      "3             estce que ton mari est aussi de boston  \n",
      "4  dans les écoles de commerce dans les couloirs ...  \n",
      "   id                                           sentence  \\\n",
      "0   0  Nous dûmes nous excuser des propos que nous eû...   \n",
      "1   1  Vous ne pouvez pas savoir le plaisir que j'ai ...   \n",
      "2   2  Et, paradoxalement, boire froid n'est pas la b...   \n",
      "3   3  Ce n'est pas étonnant, car c'est une saison my...   \n",
      "4   4  Le corps de Golo lui-même, d'une essence aussi...   \n",
      "\n",
      "                                  processed_sentence  \n",
      "0  nous dûmes nous excuser des propos que nous eû...  \n",
      "1  vous ne pouvez pas savoir le plaisir que jai d...  \n",
      "2  et paradoxalement boire froid nest pas la bonn...  \n",
      "3  ce nest pas étonnant car cest une saison mysté...  \n",
      "4  le corps de golo luimême dune essence aussi su...  \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_text(sentence):\n",
    "    # Convert to lowercase\n",
    "    sentence = sentence.lower()\n",
    "    # Remove unnecessary punctuation while retaining French accents and special characters\n",
    "    sentence = re.sub(r'[^a-zàâçéèêëîïôûùüÿñæœ\\s]', '', sentence)\n",
    "    return sentence\n",
    "\n",
    "# Apply the preprocessing to each sentence\n",
    "training_data['processed_sentence'] = training_data['sentence'].apply(preprocess_text)\n",
    "test_data['processed_sentence'] = test_data['sentence'].apply(preprocess_text)\n",
    "# Display the first few rows of the dataframe after preprocessing\n",
    "print(training_data.head())\n",
    "print(test_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id                                           sentence difficulty  \\\n",
      "0   0  Les coûts kilométriques réels peuvent diverger...         C1   \n",
      "1   1  Le bleu, c'est ma couleur préférée mais je n'a...         A1   \n",
      "2   2  Le test de niveau en français est sur le site ...         A1   \n",
      "3   3           Est-ce que ton mari est aussi de Boston?         A1   \n",
      "4   4  Dans les écoles de commerce, dans les couloirs...         B1   \n",
      "\n",
      "                                  processed_sentence  LEN  UNIQUE_WORD_COUNT  \\\n",
      "0  les coûts kilométriques réels peuvent diverger...   38                 29   \n",
      "1  le bleu cest ma couleur préférée mais je naime...   12                 11   \n",
      "2  le test de niveau en français est sur le site ...   13                 11   \n",
      "3             estce que ton mari est aussi de boston    8                  8   \n",
      "4  dans les écoles de commerce dans les couloirs ...   32                 24   \n",
      "\n",
      "   AVG_WORD_LENGTH   DCRS   FKG   ARI  ...  embedding_759  embedding_760  \\\n",
      "0         5.526316  17.57  18.1  23.6  ...      -0.067223      -0.065459   \n",
      "1         3.916667  18.71   0.9   3.0  ...      -0.152003      -0.081141   \n",
      "2         4.000000  16.43   3.6   3.9  ...      -0.074644      -0.177957   \n",
      "3         3.875000  17.85   2.9   0.8  ...      -0.096884      -0.183525   \n",
      "4         5.093750  16.57  13.4  18.5  ...      -0.074465      -0.070021   \n",
      "\n",
      "   embedding_761  embedding_762  embedding_763  embedding_764  embedding_765  \\\n",
      "0       0.134300      -0.011380       0.022304       0.008905      -0.214924   \n",
      "1       0.126171       0.053080       0.040183       0.092430      -0.128848   \n",
      "2       0.087216       0.031253       0.014485       0.073736      -0.044085   \n",
      "3       0.121947      -0.008364       0.084223       0.092719      -0.132519   \n",
      "4       0.052438       0.013443       0.033107       0.002078      -0.140131   \n",
      "\n",
      "   embedding_766  embedding_767  difficulty_encoded  \n",
      "0       0.004331      -0.054215                   5  \n",
      "1      -0.034450       0.043938                   1  \n",
      "2       0.000471      -0.027738                   1  \n",
      "3       0.058858       0.007657                   1  \n",
      "4       0.009309      -0.019106                   3  \n",
      "\n",
      "[5 rows x 780 columns]\n",
      "   id                                           sentence  \\\n",
      "0   0  Nous dûmes nous excuser des propos que nous eû...   \n",
      "1   1  Vous ne pouvez pas savoir le plaisir que j'ai ...   \n",
      "2   2  Et, paradoxalement, boire froid n'est pas la b...   \n",
      "3   3  Ce n'est pas étonnant, car c'est une saison my...   \n",
      "4   4  Le corps de Golo lui-même, d'une essence aussi...   \n",
      "\n",
      "                                  processed_sentence  LEN  UNIQUE_WORD_COUNT  \\\n",
      "0  nous dûmes nous excuser des propos que nous eû...   10                  8   \n",
      "1  vous ne pouvez pas savoir le plaisir que jai d...   14                 14   \n",
      "2  et paradoxalement boire froid nest pas la bonn...    9                  9   \n",
      "3  ce nest pas étonnant car cest une saison mysté...    9                  9   \n",
      "4  le corps de golo luimême dune essence aussi su...   72                 56   \n",
      "\n",
      "   AVG_WORD_LENGTH   DCRS   FKG   ARI  SYLLABLE_COUNT  ...  embedding_758  \\\n",
      "0         5.000000  16.76   4.8   7.1              15  ...      -0.091074   \n",
      "1         4.571429  20.12   4.0   7.1              20  ...      -0.048117   \n",
      "2         5.111111  16.36   3.3   7.1              14  ...      -0.022979   \n",
      "3         4.888889  16.36   3.3   6.1              13  ...      -0.002399   \n",
      "4         5.236111  18.83  30.2  39.3             122  ...       0.039166   \n",
      "\n",
      "   embedding_759  embedding_760  embedding_761  embedding_762  embedding_763  \\\n",
      "0      -0.057849      -0.153783       0.113769       0.082491       0.007048   \n",
      "1      -0.051733      -0.169284       0.072031       0.052745      -0.041019   \n",
      "2      -0.170090      -0.108290       0.151637       0.012537       0.052576   \n",
      "3      -0.119357      -0.086169       0.117364      -0.009256       0.032399   \n",
      "4      -0.021739      -0.130386      -0.106114       0.072774       0.005718   \n",
      "\n",
      "   embedding_764  embedding_765  embedding_766  embedding_767  \n",
      "0       0.068648      -0.106739       0.018842       0.003663  \n",
      "1       0.026564      -0.157390       0.003122       0.005944  \n",
      "2       0.072825      -0.054117      -0.000034      -0.021972  \n",
      "3       0.088059      -0.103890      -0.026876      -0.039505  \n",
      "4       0.045151      -0.037816       0.043461      -0.021040  \n",
      "\n",
      "[5 rows x 778 columns]\n"
     ]
    }
   ],
   "source": [
    "from transformers import CamembertTokenizer, CamembertModel\n",
    "import torch\n",
    "import textstat\n",
    "import pyphen\n",
    "\n",
    "# Initialize CamemBERT tokenizer and model\n",
    "tokenizer = CamembertTokenizer.from_pretrained('camembert-base')\n",
    "model = CamembertModel.from_pretrained('camembert-base')\n",
    "# Initialize Pyphen for syllable counting\n",
    "dic = pyphen.Pyphen(lang='fr')\n",
    "\n",
    "def get_camembert_embedding(sentence):\n",
    "    # Tokenize and encode the sentence for CamemBERT\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    # Get the embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # Extract the embeddings for the [CLS] token (representing the entire sentence)\n",
    "    embeddings = outputs.last_hidden_state[:, 0, :].squeeze().numpy()\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def count_syllables(word):\n",
    "    # Count the hyphens as an approximation of syllable count\n",
    "    hyphenated = dic.inserted(word)\n",
    "    return hyphenated.count('-') + 1\n",
    "\n",
    "\n",
    "# Apply feature engineering and concatenate CamemBERT embeddings\n",
    "def add_features(df):\n",
    "    # Adding features based on text properties\n",
    "    df['LEN'] = df['processed_sentence'].apply(lambda x: len(x.split()))\n",
    "    df['UNIQUE_WORD_COUNT'] = df['processed_sentence'].apply(lambda x: len(set(x.split())))\n",
    "    df['AVG_WORD_LENGTH'] = df['processed_sentence'].apply(lambda x: sum(len(word) for word in x.split()) / len(x.split()) if x.split() else 0)\n",
    "    df['DCRS'] = df['processed_sentence'].apply(lambda x: textstat.dale_chall_readability_score(x))\n",
    "    df['FKG'] = df['processed_sentence'].apply(lambda x: textstat.flesch_kincaid_grade(x))\n",
    "    df['ARI'] = df['processed_sentence'].apply(lambda x: textstat.automated_readability_index(x))\n",
    "    df['SYLLABLE_COUNT'] = df['processed_sentence'].apply(lambda x: sum(count_syllables(word) for word in x.split()))\n",
    "\n",
    "    # Adding CamemBERT embeddings\n",
    "    embeddings = df['processed_sentence'].apply(get_camembert_embedding).tolist()\n",
    "    # Generate column names for embeddings\n",
    "    embedding_column_names = [f'embedding_{i}' for i in range(len(embeddings[0]))]\n",
    "    embeddings_df = pd.DataFrame(embeddings, columns=embedding_column_names)\n",
    "    df = pd.concat([df, embeddings_df], axis=1)\n",
    "\n",
    "    return df\n",
    "# Apply feature engineering\n",
    "training_data = add_features(training_data)\n",
    "test_data = add_features(test_data)\n",
    "\n",
    "# Encoding the difficulty levels\n",
    "difficulty_encoding = {'A1': 1, 'A2': 2, 'B1': 3, 'B2': 4, 'C1': 5, 'C2': 6}\n",
    "training_data['difficulty_encoded'] = training_data['difficulty'].map(difficulty_encoding)\n",
    "\n",
    "# Display the dataframe with new features\n",
    "print(training_data.head())\n",
    "print(test_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add binairy difficulty column\n",
    "This way the difficulty is first divided in easy and difficult and later the CEFR label is given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>difficulty</th>\n",
       "      <th>processed_sentence</th>\n",
       "      <th>LEN</th>\n",
       "      <th>UNIQUE_WORD_COUNT</th>\n",
       "      <th>AVG_WORD_LENGTH</th>\n",
       "      <th>DCRS</th>\n",
       "      <th>FKG</th>\n",
       "      <th>ARI</th>\n",
       "      <th>...</th>\n",
       "      <th>embedding_760</th>\n",
       "      <th>embedding_761</th>\n",
       "      <th>embedding_762</th>\n",
       "      <th>embedding_763</th>\n",
       "      <th>embedding_764</th>\n",
       "      <th>embedding_765</th>\n",
       "      <th>embedding_766</th>\n",
       "      <th>embedding_767</th>\n",
       "      <th>difficulty_encoded</th>\n",
       "      <th>difficulty_binary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Les coûts kilométriques réels peuvent diverger...</td>\n",
       "      <td>C1</td>\n",
       "      <td>les coûts kilométriques réels peuvent diverger...</td>\n",
       "      <td>38</td>\n",
       "      <td>29</td>\n",
       "      <td>5.526316</td>\n",
       "      <td>17.57</td>\n",
       "      <td>18.1</td>\n",
       "      <td>23.6</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.065459</td>\n",
       "      <td>0.134300</td>\n",
       "      <td>-0.011380</td>\n",
       "      <td>0.022304</td>\n",
       "      <td>0.008905</td>\n",
       "      <td>-0.214924</td>\n",
       "      <td>0.004331</td>\n",
       "      <td>-0.054215</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Le bleu, c'est ma couleur préférée mais je n'a...</td>\n",
       "      <td>A1</td>\n",
       "      <td>le bleu cest ma couleur préférée mais je naime...</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>3.916667</td>\n",
       "      <td>18.71</td>\n",
       "      <td>0.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.081141</td>\n",
       "      <td>0.126171</td>\n",
       "      <td>0.053080</td>\n",
       "      <td>0.040183</td>\n",
       "      <td>0.092430</td>\n",
       "      <td>-0.128848</td>\n",
       "      <td>-0.034450</td>\n",
       "      <td>0.043938</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Le test de niveau en français est sur le site ...</td>\n",
       "      <td>A1</td>\n",
       "      <td>le test de niveau en français est sur le site ...</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>16.43</td>\n",
       "      <td>3.6</td>\n",
       "      <td>3.9</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.177957</td>\n",
       "      <td>0.087216</td>\n",
       "      <td>0.031253</td>\n",
       "      <td>0.014485</td>\n",
       "      <td>0.073736</td>\n",
       "      <td>-0.044085</td>\n",
       "      <td>0.000471</td>\n",
       "      <td>-0.027738</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Est-ce que ton mari est aussi de Boston?</td>\n",
       "      <td>A1</td>\n",
       "      <td>estce que ton mari est aussi de boston</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>3.875000</td>\n",
       "      <td>17.85</td>\n",
       "      <td>2.9</td>\n",
       "      <td>0.8</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.183525</td>\n",
       "      <td>0.121947</td>\n",
       "      <td>-0.008364</td>\n",
       "      <td>0.084223</td>\n",
       "      <td>0.092719</td>\n",
       "      <td>-0.132519</td>\n",
       "      <td>0.058858</td>\n",
       "      <td>0.007657</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Dans les écoles de commerce, dans les couloirs...</td>\n",
       "      <td>B1</td>\n",
       "      <td>dans les écoles de commerce dans les couloirs ...</td>\n",
       "      <td>32</td>\n",
       "      <td>24</td>\n",
       "      <td>5.093750</td>\n",
       "      <td>16.57</td>\n",
       "      <td>13.4</td>\n",
       "      <td>18.5</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.070021</td>\n",
       "      <td>0.052438</td>\n",
       "      <td>0.013443</td>\n",
       "      <td>0.033107</td>\n",
       "      <td>0.002078</td>\n",
       "      <td>-0.140131</td>\n",
       "      <td>0.009309</td>\n",
       "      <td>-0.019106</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>voilà une autre histoire que j'ai beaucoup aimée.</td>\n",
       "      <td>A2</td>\n",
       "      <td>voilà une autre histoire que jai beaucoup aimée</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>19.82</td>\n",
       "      <td>2.9</td>\n",
       "      <td>6.1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.136071</td>\n",
       "      <td>0.123643</td>\n",
       "      <td>0.066058</td>\n",
       "      <td>0.008499</td>\n",
       "      <td>0.042318</td>\n",
       "      <td>-0.134740</td>\n",
       "      <td>0.015552</td>\n",
       "      <td>-0.023559</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>Les médecins disent souvent qu'on doit boire u...</td>\n",
       "      <td>A2</td>\n",
       "      <td>les médecins disent souvent quon doit boire un...</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>4.466667</td>\n",
       "      <td>19.12</td>\n",
       "      <td>3.2</td>\n",
       "      <td>7.1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.077710</td>\n",
       "      <td>0.077807</td>\n",
       "      <td>0.024878</td>\n",
       "      <td>0.099138</td>\n",
       "      <td>0.013850</td>\n",
       "      <td>-0.133629</td>\n",
       "      <td>0.008823</td>\n",
       "      <td>-0.001436</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>Il est particulièrement observé chez les perso...</td>\n",
       "      <td>B2</td>\n",
       "      <td>il est particulièrement observé chez les perso...</td>\n",
       "      <td>31</td>\n",
       "      <td>27</td>\n",
       "      <td>5.612903</td>\n",
       "      <td>18.42</td>\n",
       "      <td>17.7</td>\n",
       "      <td>20.5</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.093944</td>\n",
       "      <td>0.137133</td>\n",
       "      <td>0.076429</td>\n",
       "      <td>0.055389</td>\n",
       "      <td>0.041365</td>\n",
       "      <td>-0.033010</td>\n",
       "      <td>0.055074</td>\n",
       "      <td>-0.050868</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>J'ai retrouvé le plaisir de manger un oeuf à l...</td>\n",
       "      <td>A2</td>\n",
       "      <td>jai retrouvé le plaisir de manger un oeuf à la...</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>3.818182</td>\n",
       "      <td>18.54</td>\n",
       "      <td>1.7</td>\n",
       "      <td>2.1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.137939</td>\n",
       "      <td>0.020195</td>\n",
       "      <td>0.013345</td>\n",
       "      <td>0.054082</td>\n",
       "      <td>0.047530</td>\n",
       "      <td>-0.168198</td>\n",
       "      <td>-0.061109</td>\n",
       "      <td>0.011835</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>Nous allons bien, nous habitons dans une petit...</td>\n",
       "      <td>B1</td>\n",
       "      <td>nous allons bien nous habitons dans une petite...</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>4.866667</td>\n",
       "      <td>19.12</td>\n",
       "      <td>6.8</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.168125</td>\n",
       "      <td>0.132555</td>\n",
       "      <td>0.172277</td>\n",
       "      <td>0.040058</td>\n",
       "      <td>0.115313</td>\n",
       "      <td>-0.087904</td>\n",
       "      <td>0.057913</td>\n",
       "      <td>-0.012454</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>Bonjour et bonne année.</td>\n",
       "      <td>A1</td>\n",
       "      <td>bonjour et bonne année</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4.750000</td>\n",
       "      <td>19.62</td>\n",
       "      <td>3.7</td>\n",
       "      <td>2.9</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.244061</td>\n",
       "      <td>0.177687</td>\n",
       "      <td>-0.064068</td>\n",
       "      <td>0.047858</td>\n",
       "      <td>0.141869</td>\n",
       "      <td>-0.034535</td>\n",
       "      <td>-0.035550</td>\n",
       "      <td>-0.085396</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>La presse s'est abondamment fait l'écho des ef...</td>\n",
       "      <td>B2</td>\n",
       "      <td>la presse sest abondamment fait lécho des effe...</td>\n",
       "      <td>17</td>\n",
       "      <td>16</td>\n",
       "      <td>5.176471</td>\n",
       "      <td>19.34</td>\n",
       "      <td>7.6</td>\n",
       "      <td>11.5</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.074577</td>\n",
       "      <td>0.114755</td>\n",
       "      <td>-0.005150</td>\n",
       "      <td>0.076100</td>\n",
       "      <td>0.050291</td>\n",
       "      <td>-0.199595</td>\n",
       "      <td>-0.069524</td>\n",
       "      <td>-0.008006</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>Pour que le rocher s'ouvre, il faut le toucher...</td>\n",
       "      <td>B1</td>\n",
       "      <td>pour que le rocher souvre il faut le toucher a...</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>17.64</td>\n",
       "      <td>3.6</td>\n",
       "      <td>3.9</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.127871</td>\n",
       "      <td>0.046093</td>\n",
       "      <td>0.096564</td>\n",
       "      <td>0.052534</td>\n",
       "      <td>0.035976</td>\n",
       "      <td>-0.059493</td>\n",
       "      <td>-0.024654</td>\n",
       "      <td>-0.031890</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>J'habite une belle ville dans le nord de la Fr...</td>\n",
       "      <td>A1</td>\n",
       "      <td>jhabite une belle ville dans le nord de la france</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>19.92</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.4</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.151931</td>\n",
       "      <td>0.147460</td>\n",
       "      <td>0.088496</td>\n",
       "      <td>0.048473</td>\n",
       "      <td>0.107045</td>\n",
       "      <td>-0.194075</td>\n",
       "      <td>-0.037810</td>\n",
       "      <td>0.013032</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>Certes il doit répondre aux goûts du consommat...</td>\n",
       "      <td>B2</td>\n",
       "      <td>certes il doit répondre aux goûts du consommat...</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>5.333333</td>\n",
       "      <td>18.07</td>\n",
       "      <td>9.1</td>\n",
       "      <td>11.2</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.171549</td>\n",
       "      <td>0.117540</td>\n",
       "      <td>0.013436</td>\n",
       "      <td>0.068444</td>\n",
       "      <td>0.066534</td>\n",
       "      <td>-0.079101</td>\n",
       "      <td>0.050670</td>\n",
       "      <td>-0.009214</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>Ma timidité me quittait dès que je m'éloignais...</td>\n",
       "      <td>C2</td>\n",
       "      <td>ma timidité me quittait dès que je méloignais ...</td>\n",
       "      <td>35</td>\n",
       "      <td>28</td>\n",
       "      <td>5.142857</td>\n",
       "      <td>17.55</td>\n",
       "      <td>14.6</td>\n",
       "      <td>20.3</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004759</td>\n",
       "      <td>0.064298</td>\n",
       "      <td>0.085041</td>\n",
       "      <td>0.003688</td>\n",
       "      <td>0.026600</td>\n",
       "      <td>-0.130536</td>\n",
       "      <td>-0.013226</td>\n",
       "      <td>0.093807</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>Je pense que fréquenter les galeries est la me...</td>\n",
       "      <td>B2</td>\n",
       "      <td>je pense que fréquenter les galeries est la me...</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>5.250000</td>\n",
       "      <td>19.23</td>\n",
       "      <td>7.2</td>\n",
       "      <td>11.3</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.115419</td>\n",
       "      <td>0.090308</td>\n",
       "      <td>0.023391</td>\n",
       "      <td>0.020200</td>\n",
       "      <td>0.071431</td>\n",
       "      <td>-0.065869</td>\n",
       "      <td>-0.028084</td>\n",
       "      <td>-0.016982</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>Le soir, je me cuche tôt parce que je dois êtr...</td>\n",
       "      <td>A1</td>\n",
       "      <td>le soir je me cuche tôt parce que je dois être...</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>3.733333</td>\n",
       "      <td>18.07</td>\n",
       "      <td>3.2</td>\n",
       "      <td>3.6</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.017257</td>\n",
       "      <td>0.119002</td>\n",
       "      <td>0.064054</td>\n",
       "      <td>0.023106</td>\n",
       "      <td>0.049391</td>\n",
       "      <td>-0.137551</td>\n",
       "      <td>-0.008629</td>\n",
       "      <td>0.025417</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>La mère de Raphaël et de Vanessa exprime sa sa...</td>\n",
       "      <td>B2</td>\n",
       "      <td>la mère de raphaël et de vanessa exprime sa sa...</td>\n",
       "      <td>23</td>\n",
       "      <td>22</td>\n",
       "      <td>4.913043</td>\n",
       "      <td>19.19</td>\n",
       "      <td>11.1</td>\n",
       "      <td>13.2</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.134868</td>\n",
       "      <td>0.099055</td>\n",
       "      <td>0.009676</td>\n",
       "      <td>0.007083</td>\n",
       "      <td>-0.026611</td>\n",
       "      <td>-0.141564</td>\n",
       "      <td>-0.025964</td>\n",
       "      <td>0.085937</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>Je ne fais pas grand-chose à la maison.</td>\n",
       "      <td>A2</td>\n",
       "      <td>je ne fais pas grandchose à la maison</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>3.750000</td>\n",
       "      <td>19.82</td>\n",
       "      <td>2.9</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.147107</td>\n",
       "      <td>0.147048</td>\n",
       "      <td>0.065594</td>\n",
       "      <td>0.021253</td>\n",
       "      <td>0.111708</td>\n",
       "      <td>-0.062336</td>\n",
       "      <td>-0.024811</td>\n",
       "      <td>0.014953</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 781 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                                           sentence difficulty  \\\n",
       "0    0  Les coûts kilométriques réels peuvent diverger...         C1   \n",
       "1    1  Le bleu, c'est ma couleur préférée mais je n'a...         A1   \n",
       "2    2  Le test de niveau en français est sur le site ...         A1   \n",
       "3    3           Est-ce que ton mari est aussi de Boston?         A1   \n",
       "4    4  Dans les écoles de commerce, dans les couloirs...         B1   \n",
       "5    5  voilà une autre histoire que j'ai beaucoup aimée.         A2   \n",
       "6    6  Les médecins disent souvent qu'on doit boire u...         A2   \n",
       "7    7  Il est particulièrement observé chez les perso...         B2   \n",
       "8    8  J'ai retrouvé le plaisir de manger un oeuf à l...         A2   \n",
       "9    9  Nous allons bien, nous habitons dans une petit...         B1   \n",
       "10  10                            Bonjour et bonne année.         A1   \n",
       "11  11  La presse s'est abondamment fait l'écho des ef...         B2   \n",
       "12  12  Pour que le rocher s'ouvre, il faut le toucher...         B1   \n",
       "13  13  J'habite une belle ville dans le nord de la Fr...         A1   \n",
       "14  14  Certes il doit répondre aux goûts du consommat...         B2   \n",
       "15  15  Ma timidité me quittait dès que je m'éloignais...         C2   \n",
       "16  16  Je pense que fréquenter les galeries est la me...         B2   \n",
       "17  17  Le soir, je me cuche tôt parce que je dois êtr...         A1   \n",
       "18  18  La mère de Raphaël et de Vanessa exprime sa sa...         B2   \n",
       "19  19            Je ne fais pas grand-chose à la maison.         A2   \n",
       "\n",
       "                                   processed_sentence  LEN  UNIQUE_WORD_COUNT  \\\n",
       "0   les coûts kilométriques réels peuvent diverger...   38                 29   \n",
       "1   le bleu cest ma couleur préférée mais je naime...   12                 11   \n",
       "2   le test de niveau en français est sur le site ...   13                 11   \n",
       "3              estce que ton mari est aussi de boston    8                  8   \n",
       "4   dans les écoles de commerce dans les couloirs ...   32                 24   \n",
       "5     voilà une autre histoire que jai beaucoup aimée    8                  8   \n",
       "6   les médecins disent souvent quon doit boire un...   15                 14   \n",
       "7   il est particulièrement observé chez les perso...   31                 27   \n",
       "8   jai retrouvé le plaisir de manger un oeuf à la...   11                 11   \n",
       "9   nous allons bien nous habitons dans une petite...   15                 14   \n",
       "10                             bonjour et bonne année    4                  4   \n",
       "11  la presse sest abondamment fait lécho des effe...   17                 16   \n",
       "12  pour que le rocher souvre il faut le toucher a...   13                 12   \n",
       "13  jhabite une belle ville dans le nord de la france   10                 10   \n",
       "14  certes il doit répondre aux goûts du consommat...   15                 14   \n",
       "15  ma timidité me quittait dès que je méloignais ...   35                 28   \n",
       "16  je pense que fréquenter les galeries est la me...   16                 16   \n",
       "17  le soir je me cuche tôt parce que je dois être...   15                 14   \n",
       "18  la mère de raphaël et de vanessa exprime sa sa...   23                 22   \n",
       "19              je ne fais pas grandchose à la maison    8                  8   \n",
       "\n",
       "    AVG_WORD_LENGTH   DCRS   FKG   ARI  ...  embedding_760  embedding_761  \\\n",
       "0          5.526316  17.57  18.1  23.6  ...      -0.065459       0.134300   \n",
       "1          3.916667  18.71   0.9   3.0  ...      -0.081141       0.126171   \n",
       "2          4.000000  16.43   3.6   3.9  ...      -0.177957       0.087216   \n",
       "3          3.875000  17.85   2.9   0.8  ...      -0.183525       0.121947   \n",
       "4          5.093750  16.57  13.4  18.5  ...      -0.070021       0.052438   \n",
       "5          5.000000  19.82   2.9   6.1  ...      -0.136071       0.123643   \n",
       "6          4.466667  19.12   3.2   7.1  ...      -0.077710       0.077807   \n",
       "7          5.612903  18.42  17.7  20.5  ...      -0.093944       0.137133   \n",
       "8          3.818182  18.54   1.7   2.1  ...      -0.137939       0.020195   \n",
       "9          4.866667  19.12   6.8   9.0  ...      -0.168125       0.132555   \n",
       "10         4.750000  19.62   3.7   2.9  ...      -0.244061       0.177687   \n",
       "11         5.176471  19.34   7.6  11.5  ...      -0.074577       0.114755   \n",
       "12         4.000000  17.64   3.6   3.9  ...      -0.127871       0.046093   \n",
       "13         4.000000  19.92   0.1   2.4  ...      -0.151931       0.147460   \n",
       "14         5.333333  18.07   9.1  11.2  ...      -0.171549       0.117540   \n",
       "15         5.142857  17.55  14.6  20.3  ...      -0.004759       0.064298   \n",
       "16         5.250000  19.23   7.2  11.3  ...      -0.115419       0.090308   \n",
       "17         3.733333  18.07   3.2   3.6  ...      -0.017257       0.119002   \n",
       "18         4.913043  19.19  11.1  13.2  ...      -0.134868       0.099055   \n",
       "19         3.750000  19.82   2.9   0.2  ...      -0.147107       0.147048   \n",
       "\n",
       "    embedding_762  embedding_763  embedding_764  embedding_765  embedding_766  \\\n",
       "0       -0.011380       0.022304       0.008905      -0.214924       0.004331   \n",
       "1        0.053080       0.040183       0.092430      -0.128848      -0.034450   \n",
       "2        0.031253       0.014485       0.073736      -0.044085       0.000471   \n",
       "3       -0.008364       0.084223       0.092719      -0.132519       0.058858   \n",
       "4        0.013443       0.033107       0.002078      -0.140131       0.009309   \n",
       "5        0.066058       0.008499       0.042318      -0.134740       0.015552   \n",
       "6        0.024878       0.099138       0.013850      -0.133629       0.008823   \n",
       "7        0.076429       0.055389       0.041365      -0.033010       0.055074   \n",
       "8        0.013345       0.054082       0.047530      -0.168198      -0.061109   \n",
       "9        0.172277       0.040058       0.115313      -0.087904       0.057913   \n",
       "10      -0.064068       0.047858       0.141869      -0.034535      -0.035550   \n",
       "11      -0.005150       0.076100       0.050291      -0.199595      -0.069524   \n",
       "12       0.096564       0.052534       0.035976      -0.059493      -0.024654   \n",
       "13       0.088496       0.048473       0.107045      -0.194075      -0.037810   \n",
       "14       0.013436       0.068444       0.066534      -0.079101       0.050670   \n",
       "15       0.085041       0.003688       0.026600      -0.130536      -0.013226   \n",
       "16       0.023391       0.020200       0.071431      -0.065869      -0.028084   \n",
       "17       0.064054       0.023106       0.049391      -0.137551      -0.008629   \n",
       "18       0.009676       0.007083      -0.026611      -0.141564      -0.025964   \n",
       "19       0.065594       0.021253       0.111708      -0.062336      -0.024811   \n",
       "\n",
       "    embedding_767  difficulty_encoded  difficulty_binary  \n",
       "0       -0.054215                   5                  1  \n",
       "1        0.043938                   1                  0  \n",
       "2       -0.027738                   1                  0  \n",
       "3        0.007657                   1                  0  \n",
       "4       -0.019106                   3                  0  \n",
       "5       -0.023559                   2                  0  \n",
       "6       -0.001436                   2                  0  \n",
       "7       -0.050868                   4                  1  \n",
       "8        0.011835                   2                  0  \n",
       "9       -0.012454                   3                  0  \n",
       "10      -0.085396                   1                  0  \n",
       "11      -0.008006                   4                  1  \n",
       "12      -0.031890                   3                  0  \n",
       "13       0.013032                   1                  0  \n",
       "14      -0.009214                   4                  1  \n",
       "15       0.093807                   6                  1  \n",
       "16      -0.016982                   4                  1  \n",
       "17       0.025417                   1                  0  \n",
       "18       0.085937                   4                  1  \n",
       "19       0.014953                   2                  0  \n",
       "\n",
       "[20 rows x 781 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def map_difficulty_to_binary(difficulty):\n",
    "    # Mapping lower difficulties (A1 to B1) to 0 and higher difficulties (B2 to C2) to 1\n",
    "    return 0 if difficulty in ['A1', 'A2', 'B1'] else 1\n",
    "\n",
    "# Apply the mapping to create a new boolean column\n",
    "training_data['difficulty_binary'] = training_data['difficulty'].apply(map_difficulty_to_binary)\n",
    "\n",
    "# Display the dataframe with the new column\n",
    "training_data.head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create another column to split the difficulty per letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>difficulty</th>\n",
       "      <th>processed_sentence</th>\n",
       "      <th>LEN</th>\n",
       "      <th>UNIQUE_WORD_COUNT</th>\n",
       "      <th>AVG_WORD_LENGTH</th>\n",
       "      <th>DCRS</th>\n",
       "      <th>FKG</th>\n",
       "      <th>ARI</th>\n",
       "      <th>...</th>\n",
       "      <th>embedding_761</th>\n",
       "      <th>embedding_762</th>\n",
       "      <th>embedding_763</th>\n",
       "      <th>embedding_764</th>\n",
       "      <th>embedding_765</th>\n",
       "      <th>embedding_766</th>\n",
       "      <th>embedding_767</th>\n",
       "      <th>difficulty_encoded</th>\n",
       "      <th>difficulty_binary</th>\n",
       "      <th>difficulty_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Les coûts kilométriques réels peuvent diverger...</td>\n",
       "      <td>C1</td>\n",
       "      <td>les coûts kilométriques réels peuvent diverger...</td>\n",
       "      <td>38</td>\n",
       "      <td>29</td>\n",
       "      <td>5.526316</td>\n",
       "      <td>17.57</td>\n",
       "      <td>18.1</td>\n",
       "      <td>23.6</td>\n",
       "      <td>...</td>\n",
       "      <td>0.134300</td>\n",
       "      <td>-0.011380</td>\n",
       "      <td>0.022304</td>\n",
       "      <td>0.008905</td>\n",
       "      <td>-0.214924</td>\n",
       "      <td>0.004331</td>\n",
       "      <td>-0.054215</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Le bleu, c'est ma couleur préférée mais je n'a...</td>\n",
       "      <td>A1</td>\n",
       "      <td>le bleu cest ma couleur préférée mais je naime...</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>3.916667</td>\n",
       "      <td>18.71</td>\n",
       "      <td>0.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.126171</td>\n",
       "      <td>0.053080</td>\n",
       "      <td>0.040183</td>\n",
       "      <td>0.092430</td>\n",
       "      <td>-0.128848</td>\n",
       "      <td>-0.034450</td>\n",
       "      <td>0.043938</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Le test de niveau en français est sur le site ...</td>\n",
       "      <td>A1</td>\n",
       "      <td>le test de niveau en français est sur le site ...</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>16.43</td>\n",
       "      <td>3.6</td>\n",
       "      <td>3.9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.087216</td>\n",
       "      <td>0.031253</td>\n",
       "      <td>0.014485</td>\n",
       "      <td>0.073736</td>\n",
       "      <td>-0.044085</td>\n",
       "      <td>0.000471</td>\n",
       "      <td>-0.027738</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Est-ce que ton mari est aussi de Boston?</td>\n",
       "      <td>A1</td>\n",
       "      <td>estce que ton mari est aussi de boston</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>3.875000</td>\n",
       "      <td>17.85</td>\n",
       "      <td>2.9</td>\n",
       "      <td>0.8</td>\n",
       "      <td>...</td>\n",
       "      <td>0.121947</td>\n",
       "      <td>-0.008364</td>\n",
       "      <td>0.084223</td>\n",
       "      <td>0.092719</td>\n",
       "      <td>-0.132519</td>\n",
       "      <td>0.058858</td>\n",
       "      <td>0.007657</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Dans les écoles de commerce, dans les couloirs...</td>\n",
       "      <td>B1</td>\n",
       "      <td>dans les écoles de commerce dans les couloirs ...</td>\n",
       "      <td>32</td>\n",
       "      <td>24</td>\n",
       "      <td>5.093750</td>\n",
       "      <td>16.57</td>\n",
       "      <td>13.4</td>\n",
       "      <td>18.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052438</td>\n",
       "      <td>0.013443</td>\n",
       "      <td>0.033107</td>\n",
       "      <td>0.002078</td>\n",
       "      <td>-0.140131</td>\n",
       "      <td>0.009309</td>\n",
       "      <td>-0.019106</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 782 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                           sentence difficulty  \\\n",
       "0   0  Les coûts kilométriques réels peuvent diverger...         C1   \n",
       "1   1  Le bleu, c'est ma couleur préférée mais je n'a...         A1   \n",
       "2   2  Le test de niveau en français est sur le site ...         A1   \n",
       "3   3           Est-ce que ton mari est aussi de Boston?         A1   \n",
       "4   4  Dans les écoles de commerce, dans les couloirs...         B1   \n",
       "\n",
       "                                  processed_sentence  LEN  UNIQUE_WORD_COUNT  \\\n",
       "0  les coûts kilométriques réels peuvent diverger...   38                 29   \n",
       "1  le bleu cest ma couleur préférée mais je naime...   12                 11   \n",
       "2  le test de niveau en français est sur le site ...   13                 11   \n",
       "3             estce que ton mari est aussi de boston    8                  8   \n",
       "4  dans les écoles de commerce dans les couloirs ...   32                 24   \n",
       "\n",
       "   AVG_WORD_LENGTH   DCRS   FKG   ARI  ...  embedding_761  embedding_762  \\\n",
       "0         5.526316  17.57  18.1  23.6  ...       0.134300      -0.011380   \n",
       "1         3.916667  18.71   0.9   3.0  ...       0.126171       0.053080   \n",
       "2         4.000000  16.43   3.6   3.9  ...       0.087216       0.031253   \n",
       "3         3.875000  17.85   2.9   0.8  ...       0.121947      -0.008364   \n",
       "4         5.093750  16.57  13.4  18.5  ...       0.052438       0.013443   \n",
       "\n",
       "   embedding_763  embedding_764  embedding_765  embedding_766  embedding_767  \\\n",
       "0       0.022304       0.008905      -0.214924       0.004331      -0.054215   \n",
       "1       0.040183       0.092430      -0.128848      -0.034450       0.043938   \n",
       "2       0.014485       0.073736      -0.044085       0.000471      -0.027738   \n",
       "3       0.084223       0.092719      -0.132519       0.058858       0.007657   \n",
       "4       0.033107       0.002078      -0.140131       0.009309      -0.019106   \n",
       "\n",
       "   difficulty_encoded  difficulty_binary  difficulty_group  \n",
       "0                   5                  1                 2  \n",
       "1                   1                  0                 0  \n",
       "2                   1                  0                 0  \n",
       "3                   1                  0                 0  \n",
       "4                   3                  0                 1  \n",
       "\n",
       "[5 rows x 782 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def map_difficulty_to_group(difficulty):\n",
    "    if difficulty in ['A1', 'A2']:\n",
    "        return 0\n",
    "    elif difficulty in ['B1', 'B2']:\n",
    "        return 1\n",
    "    else: # Assuming remaining are 'C1' and 'C2'\n",
    "        return 2\n",
    "\n",
    "# Apply the mapping to create the new column\n",
    "training_data['difficulty_group'] = training_data['difficulty'].apply(map_difficulty_to_group)\n",
    "\n",
    "# Display the first few rows to verify the new column\n",
    "training_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the data in a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "training_data.to_csv('Data/3model_training_data.csv', index= False)\n",
    "test_data.to_csv('Data/3model_test_data.csv', index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.read_csv('Data/3model_training_data.csv')\n",
    "test_data = pd.read_csv('Data/3model_test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id                                           sentence difficulty  \\\n",
      "0   0  Les coûts kilométriques réels peuvent diverger...         C1   \n",
      "1   1  Le bleu, c'est ma couleur préférée mais je n'a...         A1   \n",
      "2   2  Le test de niveau en français est sur le site ...         A1   \n",
      "3   3           Est-ce que ton mari est aussi de Boston?         A1   \n",
      "4   4  Dans les écoles de commerce, dans les couloirs...         B1   \n",
      "\n",
      "                                  processed_sentence  LEN  UNIQUE_WORD_COUNT  \\\n",
      "0  les coûts kilométriques réels peuvent diverger...   38                 29   \n",
      "1  le bleu cest ma couleur préférée mais je naime...   12                 11   \n",
      "2  le test de niveau en français est sur le site ...   13                 11   \n",
      "3             estce que ton mari est aussi de boston    8                  8   \n",
      "4  dans les écoles de commerce dans les couloirs ...   32                 24   \n",
      "\n",
      "   AVG_WORD_LENGTH   DCRS   FKG   ARI  ...  embedding_761  embedding_762  \\\n",
      "0         5.526316  17.57  18.1  23.6  ...       0.134300      -0.011380   \n",
      "1         3.916667  18.71   0.9   3.0  ...       0.126171       0.053080   \n",
      "2         4.000000  16.43   3.6   3.9  ...       0.087216       0.031253   \n",
      "3         3.875000  17.85   2.9   0.8  ...       0.121947      -0.008364   \n",
      "4         5.093750  16.57  13.4  18.5  ...       0.052438       0.013443   \n",
      "\n",
      "   embedding_763  embedding_764  embedding_765  embedding_766  embedding_767  \\\n",
      "0       0.022304       0.008905      -0.214924       0.004331      -0.054215   \n",
      "1       0.040183       0.092430      -0.128848      -0.034450       0.043938   \n",
      "2       0.014485       0.073736      -0.044085       0.000471      -0.027738   \n",
      "3       0.084223       0.092719      -0.132520       0.058858       0.007657   \n",
      "4       0.033107       0.002078      -0.140131       0.009309      -0.019106   \n",
      "\n",
      "   difficulty_encoded  difficulty_binary  difficulty_group  \n",
      "0                   5                  1                 2  \n",
      "1                   1                  0                 0  \n",
      "2                   1                  0                 0  \n",
      "3                   1                  0                 0  \n",
      "4                   3                  0                 1  \n",
      "\n",
      "[5 rows x 782 columns]\n",
      "   id                                           sentence  \\\n",
      "0   0  Nous dûmes nous excuser des propos que nous eû...   \n",
      "1   1  Vous ne pouvez pas savoir le plaisir que j'ai ...   \n",
      "2   2  Et, paradoxalement, boire froid n'est pas la b...   \n",
      "3   3  Ce n'est pas étonnant, car c'est une saison my...   \n",
      "4   4  Le corps de Golo lui-même, d'une essence aussi...   \n",
      "\n",
      "                                  processed_sentence  LEN  UNIQUE_WORD_COUNT  \\\n",
      "0  nous dûmes nous excuser des propos que nous eû...   10                  8   \n",
      "1  vous ne pouvez pas savoir le plaisir que jai d...   14                 14   \n",
      "2  et paradoxalement boire froid nest pas la bonn...    9                  9   \n",
      "3  ce nest pas étonnant car cest une saison mysté...    9                  9   \n",
      "4  le corps de golo luimême dune essence aussi su...   72                 56   \n",
      "\n",
      "   AVG_WORD_LENGTH   DCRS   FKG   ARI  SYLLABLE_COUNT  ...  embedding_758  \\\n",
      "0         5.000000  16.76   4.8   7.1              15  ...      -0.091074   \n",
      "1         4.571429  20.12   4.0   7.1              20  ...      -0.048117   \n",
      "2         5.111111  16.36   3.3   7.1              14  ...      -0.022979   \n",
      "3         4.888889  16.36   3.3   6.1              13  ...      -0.002399   \n",
      "4         5.236111  18.83  30.2  39.3             122  ...       0.039166   \n",
      "\n",
      "   embedding_759  embedding_760  embedding_761  embedding_762  embedding_763  \\\n",
      "0      -0.057849      -0.153783       0.113769       0.082491       0.007048   \n",
      "1      -0.051733      -0.169284       0.072031       0.052745      -0.041019   \n",
      "2      -0.170090      -0.108290       0.151637       0.012537       0.052576   \n",
      "3      -0.119357      -0.086169       0.117364      -0.009256       0.032399   \n",
      "4      -0.021739      -0.130386      -0.106114       0.072774       0.005718   \n",
      "\n",
      "   embedding_764  embedding_765  embedding_766  embedding_767  \n",
      "0       0.068648      -0.106739       0.018842       0.003663  \n",
      "1       0.026564      -0.157390       0.003122       0.005944  \n",
      "2       0.072825      -0.054117      -0.000034      -0.021972  \n",
      "3       0.088059      -0.103890      -0.026876      -0.039505  \n",
      "4       0.045151      -0.037816       0.043461      -0.021040  \n",
      "\n",
      "[5 rows x 778 columns]\n"
     ]
    }
   ],
   "source": [
    "print(training_data.head())\n",
    "print(test_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for the first training\n",
    "Now we going to train a model that can classify the training in 2 categories: easy and hard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "\n",
    "# Extract features and labels\n",
    "X = training_data.drop(['id', 'sentence', 'difficulty', 'processed_sentence', 'difficulty_encoded', 'difficulty_binary', 'difficulty_group'], axis=1)\n",
    "y = training_data['difficulty_group']\n",
    "\n",
    "# Ensure all column names are of type string\n",
    "X.columns = X.columns.astype(str)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Scores: [0.75729167 0.765625   0.74375    0.74583333 0.740625  ]\n",
      "Average CV Score: 0.7506250000000001\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Initialize the SVM classifier\n",
    "model = SVC(random_state=42)\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores = cross_val_score(model, X_scaled, y, cv=5)\n",
    "\n",
    "# Print the cross-validation results\n",
    "print(\"Cross-Validation Scores:\", cv_scores)\n",
    "print(\"Average CV Score:\", cv_scores.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "[CV] END .....................C=0.1, gamma=scale, kernel=rbf; total time=   8.8s\n",
      "[CV] END .....................C=0.1, gamma=scale, kernel=rbf; total time=   8.7s\n",
      "[CV] END .....................C=0.1, gamma=scale, kernel=rbf; total time=   8.7s\n",
      "[CV] END .....................C=0.1, gamma=scale, kernel=rbf; total time=   8.8s\n",
      "[CV] END .....................C=0.1, gamma=scale, kernel=rbf; total time=   8.7s\n",
      "[CV] END ....................C=0.1, gamma=scale, kernel=poly; total time=  10.3s\n",
      "[CV] END ....................C=0.1, gamma=scale, kernel=poly; total time=  10.0s\n",
      "[CV] END ....................C=0.1, gamma=scale, kernel=poly; total time=  10.6s\n",
      "[CV] END ....................C=0.1, gamma=scale, kernel=poly; total time=   9.4s\n",
      "[CV] END ....................C=0.1, gamma=scale, kernel=poly; total time=   9.5s\n",
      "[CV] END ......................C=0.1, gamma=auto, kernel=rbf; total time=   8.6s\n",
      "[CV] END ......................C=0.1, gamma=auto, kernel=rbf; total time=   9.0s\n",
      "[CV] END ......................C=0.1, gamma=auto, kernel=rbf; total time=   8.9s\n",
      "[CV] END ......................C=0.1, gamma=auto, kernel=rbf; total time=   8.8s\n",
      "[CV] END ......................C=0.1, gamma=auto, kernel=rbf; total time=   9.0s\n",
      "[CV] END .....................C=0.1, gamma=auto, kernel=poly; total time=  10.4s\n",
      "[CV] END .....................C=0.1, gamma=auto, kernel=poly; total time=  10.8s\n",
      "[CV] END .....................C=0.1, gamma=auto, kernel=poly; total time=  10.3s\n",
      "[CV] END .....................C=0.1, gamma=auto, kernel=poly; total time=  10.7s\n",
      "[CV] END .....................C=0.1, gamma=auto, kernel=poly; total time=  10.8s\n",
      "[CV] END .......................C=1, gamma=scale, kernel=rbf; total time=   7.2s\n",
      "[CV] END .......................C=1, gamma=scale, kernel=rbf; total time=   7.4s\n",
      "[CV] END .......................C=1, gamma=scale, kernel=rbf; total time=   7.3s\n",
      "[CV] END .......................C=1, gamma=scale, kernel=rbf; total time=   7.4s\n",
      "[CV] END .......................C=1, gamma=scale, kernel=rbf; total time=   7.7s\n",
      "[CV] END ......................C=1, gamma=scale, kernel=poly; total time=   9.2s\n",
      "[CV] END ......................C=1, gamma=scale, kernel=poly; total time=   9.1s\n",
      "[CV] END ......................C=1, gamma=scale, kernel=poly; total time=   9.1s\n",
      "[CV] END ......................C=1, gamma=scale, kernel=poly; total time=   9.4s\n",
      "[CV] END ......................C=1, gamma=scale, kernel=poly; total time=   9.2s\n",
      "[CV] END ........................C=1, gamma=auto, kernel=rbf; total time=   7.6s\n",
      "[CV] END ........................C=1, gamma=auto, kernel=rbf; total time=   7.6s\n",
      "[CV] END ........................C=1, gamma=auto, kernel=rbf; total time=   7.5s\n",
      "[CV] END ........................C=1, gamma=auto, kernel=rbf; total time=   7.6s\n",
      "[CV] END ........................C=1, gamma=auto, kernel=rbf; total time=   7.5s\n",
      "[CV] END .......................C=1, gamma=auto, kernel=poly; total time=   8.9s\n",
      "[CV] END .......................C=1, gamma=auto, kernel=poly; total time=   8.6s\n",
      "[CV] END .......................C=1, gamma=auto, kernel=poly; total time=   8.7s\n",
      "[CV] END .......................C=1, gamma=auto, kernel=poly; total time=   8.8s\n",
      "[CV] END .......................C=1, gamma=auto, kernel=poly; total time=   8.8s\n",
      "[CV] END ......................C=10, gamma=scale, kernel=rbf; total time=   9.3s\n",
      "[CV] END ......................C=10, gamma=scale, kernel=rbf; total time=   9.2s\n",
      "[CV] END ......................C=10, gamma=scale, kernel=rbf; total time=   9.1s\n",
      "[CV] END ......................C=10, gamma=scale, kernel=rbf; total time=   9.3s\n",
      "[CV] END ......................C=10, gamma=scale, kernel=rbf; total time=   9.3s\n",
      "[CV] END .....................C=10, gamma=scale, kernel=poly; total time=  10.1s\n",
      "[CV] END .....................C=10, gamma=scale, kernel=poly; total time=   9.9s\n",
      "[CV] END .....................C=10, gamma=scale, kernel=poly; total time=  10.2s\n",
      "[CV] END .....................C=10, gamma=scale, kernel=poly; total time=  10.3s\n",
      "[CV] END .....................C=10, gamma=scale, kernel=poly; total time=  10.2s\n",
      "[CV] END .......................C=10, gamma=auto, kernel=rbf; total time=  10.3s\n",
      "[CV] END .......................C=10, gamma=auto, kernel=rbf; total time=  10.1s\n",
      "[CV] END .......................C=10, gamma=auto, kernel=rbf; total time=  10.1s\n",
      "[CV] END .......................C=10, gamma=auto, kernel=rbf; total time=  10.2s\n",
      "[CV] END .......................C=10, gamma=auto, kernel=rbf; total time=  10.3s\n",
      "[CV] END ......................C=10, gamma=auto, kernel=poly; total time=  10.7s\n",
      "[CV] END ......................C=10, gamma=auto, kernel=poly; total time=  10.1s\n",
      "[CV] END ......................C=10, gamma=auto, kernel=poly; total time=  10.1s\n",
      "[CV] END ......................C=10, gamma=auto, kernel=poly; total time=  10.2s\n",
      "[CV] END ......................C=10, gamma=auto, kernel=poly; total time=   9.2s\n",
      "Best parameters: {'C': 10, 'gamma': 'auto', 'kernel': 'rbf'}\n",
      "Best cross-validation score: 0.753125\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10], \n",
    "    'gamma': ['scale', 'auto'], \n",
    "    'kernel': ['rbf', 'poly']\n",
    "}\n",
    "\n",
    "# Create a GridSearchCV object\n",
    "grid_search = GridSearchCV(SVC(random_state=42), param_grid, cv=5, scoring='accuracy', verbose=2)\n",
    "\n",
    "# Perform grid search\n",
    "grid_search.fit(X_scaled, y)\n",
    "\n",
    "# Print the best parameters and the corresponding score\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation score:\", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final model evaluation\n",
    "For the first part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.85      0.82       324\n",
      "           1       0.63      0.58      0.60       319\n",
      "           2       0.79      0.80      0.80       317\n",
      "\n",
      "    accuracy                           0.74       960\n",
      "   macro avg       0.74      0.74      0.74       960\n",
      "weighted avg       0.74      0.74      0.74       960\n",
      "\n",
      "Confusion Matrix:\n",
      "[[274  49   1]\n",
      " [ 69 185  65]\n",
      " [  2  61 254]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the final model\n",
    "final_model = SVC(**grid_search.best_params_, random_state=42)\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = final_model.predict(X_test)\n",
    "\n",
    "# Print classification report and confusion matrix\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Three part classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming final_model is your trained model and X_scaled contains all features\n",
    "group_predictions = final_model.predict(X_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sub-classificiation within each group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group 0 = A "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data for group 0 (A1-A2)\n",
    "group_0_data = training_data[(group_predictions == 0) & (training_data['difficulty_encoded'].isin([1, 2]))]\n",
    "X_group_0 = group_0_data.drop(['id', 'sentence', 'difficulty', 'processed_sentence', 'difficulty_binary', 'difficulty_group', 'difficulty_encoded'], axis=1)\n",
    "y_group_0 = group_0_data['difficulty_encoded']  # 'difficulty_encoded' with values 1 and 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     LEN  UNIQUE_WORD_COUNT  AVG_WORD_LENGTH   DCRS  FKG   ARI  \\\n",
      "1     12                 11         3.916667  18.71  0.9   3.0   \n",
      "2     13                 11         4.000000  16.43  3.6   3.9   \n",
      "3      8                  8         3.875000  17.85  2.9   0.8   \n",
      "5      8                  8         5.000000  19.82  2.9   6.1   \n",
      "6     15                 14         4.466667  19.12  3.2   7.1   \n",
      "8     11                 11         3.818182  18.54  1.7   2.1   \n",
      "10     4                  4         4.750000  19.62  3.7   2.9   \n",
      "13    10                 10         4.000000  19.92  0.1   2.4   \n",
      "17    15                 14         3.733333  18.07  3.2   3.6   \n",
      "19     8                  8         3.750000  19.82  2.9   0.2   \n",
      "24     6                  6         4.833333  19.72 -1.5   4.3   \n",
      "26    11                 10         4.636364  18.54  4.0   5.9   \n",
      "27     6                  6         4.000000  17.09  0.9   0.4   \n",
      "37     4                  4         3.500000  19.62 -2.3  -3.0   \n",
      "40     6                  6         4.000000  17.09  0.9   0.4   \n",
      "41    14                 12         3.571429  16.74  2.9   2.4   \n",
      "42     7                  7         3.857143  19.77  0.1   0.3   \n",
      "44     7                  7         3.714286  17.52  6.0  -0.6   \n",
      "46     2                  2         4.000000  19.53  2.9  -1.7   \n",
      "49    16                 16         5.000000  20.22  7.2  10.1   \n",
      "50     3                  3         3.333333  14.31 -2.7  -4.3   \n",
      "52     8                  8         5.625000  19.82  2.9   9.1   \n",
      "53     5                  5         3.600000  19.67 -1.9  -2.1   \n",
      "58    11                 11         3.636364  14.23  5.2   1.2   \n",
      "60     7                  7         5.285714  19.77  7.2   7.0   \n",
      "65     7                  7         3.571429  19.77 -1.2  -1.2   \n",
      "70     2                  2         9.000000  19.53  8.8  22.0   \n",
      "77     7                  7         3.714286  17.52  2.5  -0.6   \n",
      "81     3                  3         4.666667  19.58  5.6   2.1   \n",
      "90     8                  8         3.250000  17.85  2.9  -2.2   \n",
      "94     7                  7         5.000000  17.52  3.7   5.6   \n",
      "95     7                  7         4.000000  19.77  0.1   0.9   \n",
      "96     8                  8         4.125000  19.82  0.5   2.0   \n",
      "102    4                  3         5.500000  15.68  3.7   6.5   \n",
      "104   10                  9         3.700000  18.34  1.3   1.0   \n",
      "107   17                 15         4.235294  17.48  7.6   7.0   \n",
      "116   18                 18         4.333333  19.44  5.6   8.0   \n",
      "123   13                 13         4.692308  20.07  3.6   7.2   \n",
      "126    3                  3         4.000000  19.58  0.9  -1.2   \n",
      "129    5                  5         4.200000  19.67  0.5   0.9   \n",
      "136   19                 19         4.105263  19.54  7.2   7.4   \n",
      "140   14                 13         4.142857  18.99  4.0   5.1   \n",
      "141   10                 10         4.300000  19.92  3.7   3.8   \n",
      "145    9                  9         4.222222  18.12  3.3   2.9   \n",
      "146    3                  3         4.000000  19.58  5.6  -1.2   \n",
      "150    4                  4         3.750000  15.68 -2.3  -1.9   \n",
      "152    7                  7         5.285714  19.77  7.2   7.0   \n",
      "153   12                  9         4.000000  14.76  2.1   3.4   \n",
      "156    5                  5         3.800000  19.67  2.9  -1.1   \n",
      "158    7                  7         4.285714  19.77  6.0   2.3   \n",
      "\n",
      "     SYLLABLE_COUNT  embedding_0  embedding_1  embedding_2  ...  \\\n",
      "1                15    -0.095069     0.057983     0.012641  ...   \n",
      "2                18     0.003062     0.088608     0.072793  ...   \n",
      "3                11    -0.024926     0.117334     0.082324  ...   \n",
      "5                12    -0.010600     0.145767     0.042666  ...   \n",
      "6                19     0.017836     0.045484     0.099200  ...   \n",
      "8                15     0.033543     0.033628     0.134002  ...   \n",
      "10                6    -0.044622     0.137044     0.131238  ...   \n",
      "13               11    -0.041041     0.189151     0.042623  ...   \n",
      "17               16    -0.032963     0.036150     0.164653  ...   \n",
      "19               10     0.028067     0.010850     0.160231  ...   \n",
      "24                8     0.007671     0.061302     0.107473  ...   \n",
      "26               17    -0.021734     0.089822     0.142291  ...   \n",
      "27                7     0.025985     0.157997     0.159267  ...   \n",
      "37                4    -0.031581     0.059785     0.160075  ...   \n",
      "40                8     0.026395     0.031028     0.100026  ...   \n",
      "41               16    -0.027487    -0.043212     0.067199  ...   \n",
      "42                9     0.036730     0.107214     0.101090  ...   \n",
      "44               11    -0.033051     0.027849     0.077899  ...   \n",
      "46                2    -0.080146     0.283901     0.000112  ...   \n",
      "49               27    -0.048771     0.086677     0.002364  ...   \n",
      "50                3     0.022350     0.066243     0.123552  ...   \n",
      "52               14    -0.000943     0.020507     0.095337  ...   \n",
      "53                5    -0.018285     0.162928     0.108292  ...   \n",
      "58               15     0.051148    -0.019305     0.083432  ...   \n",
      "60               12    -0.057144     0.186011     0.105633  ...   \n",
      "65                7    -0.035041     0.137256     0.108479  ...   \n",
      "70                6    -0.090371     0.186380     0.148954  ...   \n",
      "77                9     0.017432     0.042699     0.062723  ...   \n",
      "81                4    -0.042456     0.123125     0.043263  ...   \n",
      "90               10    -0.015252     0.001832     0.099575  ...   \n",
      "94               10    -0.014963     0.091508     0.169696  ...   \n",
      "95               10     0.025483    -0.039363     0.104507  ...   \n",
      "96                9     0.016827     0.027679     0.146766  ...   \n",
      "102               7    -0.041014     0.091761     0.205244  ...   \n",
      "104              12    -0.032062     0.179307     0.095765  ...   \n",
      "107              22    -0.010152     0.082794     0.092874  ...   \n",
      "116              22    -0.053579     0.078090     0.057875  ...   \n",
      "123              18    -0.080616     0.206394     0.069925  ...   \n",
      "126               4     0.003207     0.045299     0.118018  ...   \n",
      "129               7    -0.048047     0.086539     0.118985  ...   \n",
      "136              26    -0.008487     0.039909     0.126678  ...   \n",
      "140              15    -0.047702     0.026977     0.008030  ...   \n",
      "141              12    -0.007126     0.128687     0.063809  ...   \n",
      "145              11     0.078670     0.063111     0.098264  ...   \n",
      "146               4    -0.081765     0.109435     0.121070  ...   \n",
      "150               4    -0.057715     0.140878     0.144751  ...   \n",
      "152              12     0.066600     0.047261     0.175743  ...   \n",
      "153              16    -0.000426     0.059547     0.151991  ...   \n",
      "156               5    -0.006338     0.167354     0.136292  ...   \n",
      "158              10    -0.022080     0.090228     0.096159  ...   \n",
      "\n",
      "     embedding_758  embedding_759  embedding_760  embedding_761  \\\n",
      "1        -0.033817      -0.152003      -0.081141       0.126171   \n",
      "2        -0.036582      -0.074644      -0.177957       0.087216   \n",
      "3        -0.042895      -0.096884      -0.183525       0.121947   \n",
      "5        -0.057058      -0.025070      -0.136071       0.123643   \n",
      "6         0.016805      -0.132339      -0.077710       0.077807   \n",
      "8        -0.096516      -0.110822      -0.137939       0.020195   \n",
      "10       -0.075117      -0.046795      -0.244061       0.177687   \n",
      "13       -0.063068      -0.100166      -0.151931       0.147460   \n",
      "17       -0.060569      -0.083169      -0.017257       0.119002   \n",
      "19       -0.069510      -0.100394      -0.147107       0.147048   \n",
      "24       -0.042815      -0.142588      -0.182377       0.123998   \n",
      "26        0.010383      -0.056547      -0.136268       0.120505   \n",
      "27       -0.042921      -0.054102      -0.086353       0.172047   \n",
      "37       -0.091735      -0.104181      -0.071364       0.191154   \n",
      "40       -0.066951       0.019730      -0.210989       0.140030   \n",
      "41       -0.046703       0.017292      -0.163367       0.035188   \n",
      "42       -0.065365      -0.013189      -0.134397       0.137809   \n",
      "44       -0.002134      -0.035919      -0.156933       0.133098   \n",
      "46       -0.013173      -0.184445      -0.071071       0.101598   \n",
      "49        0.034762      -0.150332      -0.076571       0.105349   \n",
      "50       -0.074228      -0.056840      -0.135247       0.203069   \n",
      "52       -0.056064      -0.006228      -0.090580       0.079090   \n",
      "53       -0.081114      -0.069540      -0.223837       0.081175   \n",
      "58       -0.101502      -0.094281      -0.137728       0.100127   \n",
      "60       -0.087091      -0.057120      -0.179534       0.107256   \n",
      "65       -0.084466      -0.012718      -0.180123       0.106508   \n",
      "70       -0.044811      -0.134229      -0.143587       0.128872   \n",
      "77       -0.033918      -0.007457      -0.158057       0.181396   \n",
      "81       -0.038188      -0.019902      -0.214341       0.208035   \n",
      "90       -0.041522      -0.074437      -0.143299       0.118005   \n",
      "94       -0.117165      -0.038104      -0.206569       0.106040   \n",
      "95       -0.043649      -0.103904      -0.134992       0.169677   \n",
      "96       -0.103191      -0.036591      -0.123305       0.117904   \n",
      "102      -0.076022      -0.156943      -0.208120       0.198323   \n",
      "104      -0.071847      -0.058877      -0.191126       0.139009   \n",
      "107      -0.013470      -0.023922      -0.054999       0.128166   \n",
      "116      -0.024962      -0.070512      -0.109489       0.099224   \n",
      "123      -0.052158      -0.070772      -0.045073       0.110397   \n",
      "126      -0.095418       0.008801      -0.220032       0.147696   \n",
      "129      -0.083670       0.061014      -0.100961       0.108068   \n",
      "136       0.036963      -0.000870      -0.052934       0.049735   \n",
      "140      -0.121617      -0.085336      -0.120361       0.092518   \n",
      "141      -0.071775      -0.100581      -0.141624       0.147095   \n",
      "145      -0.051965      -0.080306      -0.152715       0.179884   \n",
      "146      -0.068766      -0.126207      -0.201920       0.188190   \n",
      "150      -0.069762      -0.083389      -0.109953       0.179390   \n",
      "152      -0.049286      -0.099594      -0.186489       0.128754   \n",
      "153      -0.103537      -0.069589      -0.129333       0.146484   \n",
      "156      -0.076875      -0.015862      -0.080947       0.188649   \n",
      "158      -0.077083      -0.133746      -0.221807       0.073845   \n",
      "\n",
      "     embedding_762  embedding_763  embedding_764  embedding_765  \\\n",
      "1         0.053080       0.040183       0.092430      -0.128848   \n",
      "2         0.031253       0.014485       0.073736      -0.044085   \n",
      "3        -0.008364       0.084223       0.092719      -0.132520   \n",
      "5         0.066058       0.008499       0.042318      -0.134740   \n",
      "6         0.024878       0.099138       0.013850      -0.133629   \n",
      "8         0.013345       0.054082       0.047530      -0.168198   \n",
      "10       -0.064068       0.047858       0.141869      -0.034535   \n",
      "13        0.088496       0.048473       0.107045      -0.194075   \n",
      "17        0.064054       0.023106       0.049391      -0.137551   \n",
      "19        0.065594       0.021253       0.111708      -0.062336   \n",
      "24       -0.023512       0.045367       0.104859      -0.063630   \n",
      "26        0.048264       0.031643       0.068899      -0.106578   \n",
      "27        0.069329       0.033972       0.054978      -0.099248   \n",
      "37        0.003642       0.037308       0.051766      -0.084732   \n",
      "40       -0.095918      -0.008048       0.114265      -0.106973   \n",
      "41        0.014864      -0.000735       0.005627      -0.080854   \n",
      "42        0.046792       0.037008       0.044302      -0.142195   \n",
      "44        0.077324       0.014872       0.118494      -0.169321   \n",
      "46       -0.001549       0.065483       0.116600      -0.059831   \n",
      "49       -0.002673      -0.031732       0.035314      -0.120241   \n",
      "50        0.083307       0.066691       0.142833      -0.134590   \n",
      "52        0.189017       0.042801       0.044981      -0.010965   \n",
      "53       -0.071512       0.060275       0.044393      -0.060458   \n",
      "58       -0.007304       0.063135       0.049956      -0.040572   \n",
      "60        0.107663      -0.015781       0.112233      -0.129745   \n",
      "65       -0.035820       0.106013       0.073954      -0.063845   \n",
      "70       -0.113897       0.155687       0.052814      -0.041998   \n",
      "77       -0.035572       0.038309       0.053468      -0.089472   \n",
      "81       -0.020827       0.071093       0.139440      -0.057015   \n",
      "90       -0.007474       0.085502       0.026637      -0.128729   \n",
      "94        0.017690      -0.037352       0.069128       0.022423   \n",
      "95        0.004801       0.072461       0.117456      -0.083310   \n",
      "96        0.008325       0.064189       0.079664      -0.127738   \n",
      "102      -0.087470       0.055829       0.073248      -0.086627   \n",
      "104       0.079151       0.020347       0.053292      -0.132479   \n",
      "107       0.018243       0.106381       0.032497      -0.059816   \n",
      "116       0.025733       0.010873       0.042873      -0.146542   \n",
      "123       0.006542       0.035613       0.064140      -0.206649   \n",
      "126      -0.002900       0.051469       0.135230      -0.076377   \n",
      "129      -0.008515       0.062247       0.100861      -0.079570   \n",
      "136       0.056399       0.064538      -0.008037      -0.081946   \n",
      "140       0.043326       0.079088       0.099213      -0.090067   \n",
      "141       0.033543       0.090623       0.118684      -0.028733   \n",
      "145       0.047297      -0.002228       0.094625      -0.022560   \n",
      "146      -0.072972       0.057333       0.129556      -0.084830   \n",
      "150       0.046093       0.019498       0.117559      -0.152048   \n",
      "152       0.005187       0.032471       0.133352      -0.077927   \n",
      "153       0.096964       0.089881       0.093654      -0.103667   \n",
      "156       0.031642       0.038483       0.077875      -0.079108   \n",
      "158      -0.050477       0.090804       0.062240      -0.064191   \n",
      "\n",
      "     embedding_766  embedding_767  \n",
      "1        -0.034450       0.043938  \n",
      "2         0.000471      -0.027738  \n",
      "3         0.058858       0.007657  \n",
      "5         0.015552      -0.023559  \n",
      "6         0.008823      -0.001436  \n",
      "8        -0.061109       0.011835  \n",
      "10       -0.035550      -0.085396  \n",
      "13       -0.037810       0.013032  \n",
      "17       -0.008629       0.025417  \n",
      "19       -0.024811       0.014953  \n",
      "24       -0.009986      -0.033778  \n",
      "26        0.005632      -0.026694  \n",
      "27       -0.046508      -0.014044  \n",
      "37       -0.057019       0.044473  \n",
      "40       -0.022990      -0.044890  \n",
      "41        0.039061      -0.069727  \n",
      "42       -0.017134      -0.005044  \n",
      "44        0.007990      -0.030043  \n",
      "46       -0.064511      -0.071286  \n",
      "49       -0.031563      -0.017881  \n",
      "50       -0.019403      -0.076014  \n",
      "52       -0.035959       0.002440  \n",
      "53        0.037995       0.008983  \n",
      "58        0.068320      -0.044310  \n",
      "60       -0.021428      -0.041767  \n",
      "65        0.002066       0.025255  \n",
      "70       -0.075654       0.024089  \n",
      "77        0.035684      -0.047465  \n",
      "81       -0.040620      -0.044101  \n",
      "90       -0.029214      -0.018115  \n",
      "94       -0.083628      -0.033198  \n",
      "95        0.009829       0.019018  \n",
      "96       -0.007807       0.030638  \n",
      "102      -0.060054       0.015857  \n",
      "104      -0.025543       0.027651  \n",
      "107      -0.021850       0.024076  \n",
      "116       0.003430       0.034862  \n",
      "123      -0.087652       0.066446  \n",
      "126       0.016438      -0.013107  \n",
      "129       0.043632      -0.090423  \n",
      "136      -0.040981       0.054476  \n",
      "140       0.064744      -0.045427  \n",
      "141      -0.007440      -0.054149  \n",
      "145       0.016353      -0.006038  \n",
      "146      -0.001816      -0.003926  \n",
      "150      -0.115647      -0.044918  \n",
      "152      -0.033084      -0.027030  \n",
      "153       0.005780      -0.023846  \n",
      "156      -0.048306       0.022354  \n",
      "158      -0.008675      -0.060147  \n",
      "\n",
      "[50 rows x 775 columns]\n"
     ]
    }
   ],
   "source": [
    "print(X_group_0.head(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for group 0 (A1-A2): 0.7339743589743589\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_group_0_scaled = scaler.transform(X_group_0)  # Use the same scaler as before\n",
    "\n",
    "# Train-test split for group 0\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_group_0_scaled, y_group_0, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train a new model for group 0\n",
    "model_group_0 = SVC(**grid_search.best_params_,random_state=42)\n",
    "model_group_0.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model for group 0\n",
    "y_pred_group_0 = model_group_0.predict(X_test)\n",
    "print(\"Accuracy for group 0 (A1-A2):\", accuracy_score(y_test, y_pred_group_0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group 1 = B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data for group 1 (B1-B2)\n",
    "group_1_data = training_data[(group_predictions == 1) & (training_data['difficulty_encoded'].isin([3, 4]))]\n",
    "X_group_1 = group_1_data.drop(['id', 'sentence', 'difficulty', 'processed_sentence', 'difficulty_binary', 'difficulty_group', 'difficulty_encoded'], axis=1)\n",
    "y_group_1 = group_1_data['difficulty_encoded']  # 'difficulty_encoded' with values 3 and 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     LEN  UNIQUE_WORD_COUNT  AVG_WORD_LENGTH   DCRS   FKG   ARI  \\\n",
      "4     32                 24         5.093750  16.57  13.4  18.5   \n",
      "7     31                 27         5.612903  18.42  17.7  20.5   \n",
      "9     15                 14         4.866667  19.12   6.8   9.0   \n",
      "11    17                 16         5.176471  19.34   7.6  11.5   \n",
      "14    15                 14         5.333333  18.07   9.1  11.2   \n",
      "16    16                 16         5.250000  19.23   7.2  11.3   \n",
      "18    23                 22         4.913043  19.19  11.1  13.2   \n",
      "20     9                  9         5.777778  18.12  11.5  10.3   \n",
      "28    10                 10         4.300000  18.34   6.0   3.8   \n",
      "32    16                 16         5.250000  20.22   7.2  11.3   \n",
      "35    38                 29         5.078947  17.16  16.9  21.5   \n",
      "36    14                 13         5.142857  18.99   5.2   9.8   \n",
      "47    13                 12         5.153846  18.86   6.0   9.3   \n",
      "51    26                 25         5.000000  18.89  13.4  15.1   \n",
      "55    20                 18         4.450000  18.05   7.6   9.5   \n",
      "56    17                 15         5.705882  17.48   8.7  14.0   \n",
      "64    17                 15         6.000000  18.41   9.9  15.3   \n",
      "71    27                 24         5.740741  19.01  15.0  19.1   \n",
      "76     5                  5         4.400000  19.67   5.2   1.8   \n",
      "78     9                  9         4.444444  19.87   3.3   4.0   \n",
      "80    13                 13         6.230769  20.07   9.5  14.4   \n",
      "87    11                 11         6.454545  19.97   6.4  14.4   \n",
      "88    20                 18         4.350000  18.84   7.6   9.1   \n",
      "92     4                  4         6.250000  19.62  -2.3  10.0   \n",
      "93    15                 14         6.133333  19.12   9.1  14.9   \n",
      "97    14                 14         3.500000  18.99   2.9   2.1   \n",
      "98    37                 29         3.945946  17.42  13.0  15.7   \n",
      "100   24                 23         5.000000  19.30  11.5  14.1   \n",
      "101    8                  8         4.625000  17.85   0.5   4.4   \n",
      "103   15                 14         4.133333  18.07   5.6   5.5   \n",
      "106   14                 14         4.357143  20.12   2.9   6.1   \n",
      "108   24                 23         5.833333  19.30   9.1  18.0   \n",
      "110    9                  9         5.888889  19.87   9.2  10.8   \n",
      "113   26                 21         5.730769  17.68  11.1  18.6   \n",
      "114   32                 30         4.750000  19.53  12.2  16.9   \n",
      "117   12                 12         5.666667  20.02   4.4  11.3   \n",
      "121   33                 29         5.181818  18.19  15.0  19.5   \n",
      "127   13                 13         5.230769  18.86   8.4   9.7   \n",
      "130   14                 14         4.642857  18.99   6.4   7.4   \n",
      "131    6                  6         4.666667  14.46   2.1   3.6   \n",
      "132   17                 16         4.823529  17.48   6.4   9.8   \n",
      "133   25                 23         5.160000  18.14  13.0  15.4   \n",
      "139   21                 20         5.000000  19.72   7.9  12.6   \n",
      "142   39                 34         5.871795  18.93  17.3  25.7   \n",
      "148    2                  2         6.500000  19.53  -3.1  10.2   \n",
      "169   23                 23         6.000000  20.57  15.8  18.3   \n",
      "170   17                 15         4.470588  18.41   7.6   8.1   \n",
      "175   26                 24         4.615385  19.50   9.9  13.3   \n",
      "178   12                 11         5.333333  18.71   6.8   9.7   \n",
      "180   23                 21         4.913043  17.13   8.7  13.2   \n",
      "\n",
      "     SYLLABLE_COUNT  embedding_0  embedding_1  embedding_2  ...  \\\n",
      "4                45    -0.011324     0.112263    -0.044635  ...   \n",
      "7                59     0.045569    -0.033591    -0.010334  ...   \n",
      "9                22    -0.026642     0.217605     0.076196  ...   \n",
      "11               26    -0.007361     0.027082    -0.004431  ...   \n",
      "14               25     0.081356     0.036253     0.034131  ...   \n",
      "16               25    -0.025714     0.135909     0.105971  ...   \n",
      "18               39     0.033549     0.113330     0.039968  ...   \n",
      "20               16     0.010325     0.107710     0.112809  ...   \n",
      "28               13    -0.076717     0.202138     0.138108  ...   \n",
      "32               24     0.008200     0.163613     0.127722  ...   \n",
      "35               58     0.053539     0.178231     0.031376  ...   \n",
      "36               18    -0.013457     0.221379     0.010416  ...   \n",
      "47               20     0.017630     0.153650     0.079470  ...   \n",
      "51               44    -0.016800     0.154804     0.062991  ...   \n",
      "55               28     0.086199     0.161179     0.091198  ...   \n",
      "56               32    -0.018071     0.240770     0.117100  ...   \n",
      "64               34     0.026199     0.113461    -0.087075  ...   \n",
      "71               52    -0.020393     0.053338     0.068038  ...   \n",
      "76                8     0.073706     0.124290     0.183110  ...   \n",
      "78               15    -0.006907    -0.012908     0.154816  ...   \n",
      "80               25     0.030707     0.051323     0.127353  ...   \n",
      "87               21    -0.028271     0.138517     0.071568  ...   \n",
      "88               28    -0.059622     0.205900     0.075789  ...   \n",
      "92                5     0.079117     0.042935     0.076389  ...   \n",
      "93               27    -0.024077     0.069110    -0.030288  ...   \n",
      "97               16     0.004939     0.098189     0.058559  ...   \n",
      "98               44    -0.082243     0.186180     0.046684  ...   \n",
      "100              37     0.014314    -0.026718     0.080274  ...   \n",
      "101              12    -0.000428     0.012187     0.111260  ...   \n",
      "103              20     0.036518     0.096183     0.061017  ...   \n",
      "106              18    -0.031610     0.125388     0.114687  ...   \n",
      "108              40     0.016636    -0.029162     0.040211  ...   \n",
      "110              16     0.007802     0.080341     0.049382  ...   \n",
      "113              40     0.012132     0.110521     0.085052  ...   \n",
      "114              45     0.019256    -0.044553     0.093892  ...   \n",
      "117              20    -0.033933     0.081910     0.034176  ...   \n",
      "121              52     0.114744     0.159671     0.027380  ...   \n",
      "127              21    -0.005153     0.118386     0.102805  ...   \n",
      "130              21    -0.044341     0.185307     0.059359  ...   \n",
      "131               8    -0.095271     0.190958     0.068135  ...   \n",
      "132              23    -0.055637     0.129985     0.085224  ...   \n",
      "133              43    -0.000735     0.142398     0.128776  ...   \n",
      "139              32    -0.021038     0.104476     0.123978  ...   \n",
      "142              59     0.024668     0.103424    -0.098893  ...   \n",
      "148               3     0.039389     0.085728     0.078788  ...   \n",
      "169              45     0.032494     0.052635     0.055767  ...   \n",
      "170              25     0.021041     0.195410     0.108645  ...   \n",
      "175              36     0.035758    -0.057056     0.061363  ...   \n",
      "178              16    -0.025493     0.128322     0.089281  ...   \n",
      "180              34     0.051300    -0.030041     0.106706  ...   \n",
      "\n",
      "     embedding_758  embedding_759  embedding_760  embedding_761  \\\n",
      "4         0.044811      -0.074465      -0.070021       0.052438   \n",
      "7         0.010441      -0.143153      -0.093944       0.137133   \n",
      "9        -0.037123      -0.060620      -0.168125       0.132555   \n",
      "11       -0.003797      -0.047994      -0.074577       0.114755   \n",
      "14       -0.087069      -0.101242      -0.171549       0.117540   \n",
      "16        0.029422      -0.223780      -0.115419       0.090308   \n",
      "18       -0.035436      -0.046726      -0.134868       0.099055   \n",
      "20       -0.041301       0.035875      -0.117726       0.093852   \n",
      "28       -0.001037      -0.055402      -0.105982       0.070712   \n",
      "32        0.035606      -0.087447      -0.197992       0.074321   \n",
      "35       -0.034705      -0.193984      -0.128721       0.027056   \n",
      "36        0.020718      -0.166373      -0.170287       0.087653   \n",
      "47       -0.061245      -0.058547      -0.114947       0.091749   \n",
      "51        0.053315      -0.108733      -0.167685       0.068362   \n",
      "55       -0.046110      -0.072691      -0.108898       0.015257   \n",
      "56       -0.098877      -0.082949      -0.081271       0.065776   \n",
      "64        0.076389       0.001332      -0.094336       0.100293   \n",
      "71        0.047007      -0.193469      -0.042390       0.040294   \n",
      "76       -0.147055      -0.090447      -0.161222       0.223111   \n",
      "78        0.023785      -0.017813      -0.065937       0.079191   \n",
      "80       -0.088576      -0.079285      -0.120194       0.161216   \n",
      "87       -0.031672      -0.088403      -0.114109       0.128745   \n",
      "88       -0.044011      -0.099404      -0.275231      -0.036795   \n",
      "92       -0.074395       0.040494      -0.101449       0.164129   \n",
      "93       -0.023941      -0.114284      -0.124398       0.039478   \n",
      "97        0.029271      -0.001463      -0.148489       0.093006   \n",
      "98        0.019212      -0.029437      -0.082682       0.083244   \n",
      "100       0.048764      -0.122891      -0.102130       0.093130   \n",
      "101      -0.046659      -0.095313      -0.088226       0.076453   \n",
      "103      -0.067933      -0.067557      -0.193304       0.078911   \n",
      "106      -0.077707      -0.040880      -0.176305       0.170885   \n",
      "108      -0.017219      -0.000165      -0.060792       0.039685   \n",
      "110      -0.067850      -0.134071      -0.163326       0.085094   \n",
      "113      -0.052061      -0.049964      -0.080156       0.061849   \n",
      "114       0.006821      -0.088084      -0.130084      -0.000174   \n",
      "117      -0.000115      -0.027874      -0.184254       0.044245   \n",
      "121      -0.071896      -0.079494      -0.157083       0.063057   \n",
      "127      -0.082870      -0.006924      -0.143821       0.105909   \n",
      "130       0.035713      -0.059933      -0.084840       0.102644   \n",
      "131       0.032265      -0.089162      -0.066241      -0.034997   \n",
      "132      -0.039940      -0.068362      -0.138706       0.091319   \n",
      "133      -0.014439      -0.110562      -0.135794       0.091848   \n",
      "139      -0.050009      -0.021528      -0.139571       0.156643   \n",
      "142       0.057659      -0.081444      -0.139968      -0.013024   \n",
      "148      -0.115300      -0.013338      -0.146984       0.199326   \n",
      "169       0.026589      -0.036936      -0.074756       0.110848   \n",
      "170      -0.069702      -0.168672      -0.176286       0.071045   \n",
      "175       0.139092       0.112157      -0.119198       0.080456   \n",
      "178      -0.020029      -0.091620      -0.065808       0.036581   \n",
      "180      -0.062404      -0.153519      -0.239636       0.127113   \n",
      "\n",
      "     embedding_762  embedding_763  embedding_764  embedding_765  \\\n",
      "4         0.013443       0.033107       0.002078      -0.140131   \n",
      "7         0.076429       0.055389       0.041365      -0.033010   \n",
      "9         0.172277       0.040058       0.115313      -0.087904   \n",
      "11       -0.005150       0.076100       0.050291      -0.199595   \n",
      "14        0.013436       0.068444       0.066534      -0.079101   \n",
      "16        0.023391       0.020200       0.071431      -0.065869   \n",
      "18        0.009676       0.007083      -0.026611      -0.141564   \n",
      "20       -0.030474       0.060111       0.080916      -0.085683   \n",
      "28        0.035572       0.001394       0.024122      -0.126149   \n",
      "32        0.032530       0.048572       0.032003      -0.123649   \n",
      "35        0.168372       0.003762      -0.030012      -0.119988   \n",
      "36        0.054725       0.124764       0.115569      -0.052979   \n",
      "47       -0.088332       0.007869       0.044509      -0.108147   \n",
      "51        0.040244       0.057121      -0.071955      -0.115131   \n",
      "55       -0.013638       0.078039      -0.016682      -0.130841   \n",
      "56        0.061698       0.145655       0.052126      -0.018212   \n",
      "64        0.000817       0.071862       0.026133      -0.191554   \n",
      "71        0.069819      -0.007792      -0.052760      -0.124451   \n",
      "76       -0.027696       0.071905       0.122745       0.006449   \n",
      "78       -0.010372       0.062254       0.074710      -0.122488   \n",
      "80        0.006429       0.026915       0.046267      -0.093610   \n",
      "87       -0.017866       0.066406       0.041776      -0.049014   \n",
      "88        0.081998       0.028224      -0.009923      -0.043669   \n",
      "92        0.002384       0.093089       0.114919      -0.104713   \n",
      "93        0.126696      -0.031665       0.056954      -0.116711   \n",
      "97        0.006157       0.031575       0.030151      -0.111848   \n",
      "98        0.013826      -0.006143      -0.087906      -0.071747   \n",
      "100      -0.066222       0.022584      -0.013941      -0.137159   \n",
      "101       0.081473       0.055257       0.097641      -0.073951   \n",
      "103      -0.023670       0.068026       0.019800      -0.117697   \n",
      "106       0.000025       0.046662       0.038263      -0.086930   \n",
      "108       0.029830       0.055952      -0.017048      -0.115226   \n",
      "110       0.034891       0.066684       0.113617      -0.075510   \n",
      "113      -0.029455       0.067913       0.002853      -0.110327   \n",
      "114       0.028092      -0.007476       0.000397      -0.138993   \n",
      "117       0.076116      -0.003414       0.108950      -0.144186   \n",
      "121       0.060812       0.027198       0.002331      -0.137152   \n",
      "127       0.044478      -0.014118       0.051661      -0.068699   \n",
      "130      -0.030174       0.104497       0.004880      -0.123572   \n",
      "131      -0.031318       0.013406       0.079683      -0.038855   \n",
      "132       0.013140      -0.081566      -0.009968      -0.119382   \n",
      "133       0.053488       0.011033       0.010830      -0.125854   \n",
      "139      -0.035291       0.078111       0.075308      -0.021940   \n",
      "142      -0.039576       0.033708       0.000543      -0.136470   \n",
      "148      -0.047509       0.081256       0.100674      -0.087686   \n",
      "169      -0.040180       0.016584       0.008498      -0.156594   \n",
      "170       0.050505      -0.001426       0.000183      -0.113567   \n",
      "175       0.020287       0.045034      -0.016736      -0.117130   \n",
      "178      -0.040905       0.008021       0.100999      -0.101396   \n",
      "180      -0.030196      -0.018400       0.029892      -0.149572   \n",
      "\n",
      "     embedding_766  embedding_767  \n",
      "4         0.009309      -0.019106  \n",
      "7         0.055074      -0.050868  \n",
      "9         0.057913      -0.012454  \n",
      "11       -0.069524      -0.008006  \n",
      "14        0.050670      -0.009214  \n",
      "16       -0.028084      -0.016982  \n",
      "18       -0.025964       0.085937  \n",
      "20       -0.016872      -0.074245  \n",
      "28       -0.156763      -0.039170  \n",
      "32       -0.045030       0.009895  \n",
      "35       -0.013429       0.060006  \n",
      "36       -0.017503       0.009406  \n",
      "47        0.024410      -0.003745  \n",
      "51       -0.033222      -0.006318  \n",
      "55       -0.032123      -0.056158  \n",
      "56        0.045231      -0.017373  \n",
      "64       -0.047084      -0.041241  \n",
      "71       -0.083578       0.086006  \n",
      "76        0.001208      -0.062183  \n",
      "78        0.011247      -0.030206  \n",
      "80        0.069431      -0.017068  \n",
      "87        0.003695      -0.032824  \n",
      "88       -0.024800       0.005184  \n",
      "92       -0.023801      -0.063389  \n",
      "93       -0.033633       0.007655  \n",
      "97       -0.036575      -0.001532  \n",
      "98        0.040627       0.082291  \n",
      "100       0.050628      -0.030602  \n",
      "101      -0.040677       0.059145  \n",
      "103       0.080969      -0.058006  \n",
      "106       0.057742       0.033982  \n",
      "108      -0.034498       0.017544  \n",
      "110      -0.021564       0.011904  \n",
      "113      -0.005247       0.046519  \n",
      "114      -0.006037       0.018070  \n",
      "117       0.011641      -0.047852  \n",
      "121      -0.002754      -0.027530  \n",
      "127       0.022761      -0.017089  \n",
      "130      -0.026947      -0.014394  \n",
      "131      -0.115819      -0.053919  \n",
      "132       0.054458       0.017412  \n",
      "133      -0.036929       0.034874  \n",
      "139       0.050191      -0.071543  \n",
      "142       0.054866      -0.050761  \n",
      "148      -0.075333      -0.019626  \n",
      "169       0.021178      -0.028509  \n",
      "170       0.036537      -0.026628  \n",
      "175       0.036174       0.030950  \n",
      "178      -0.079131      -0.002528  \n",
      "180       0.104090       0.023126  \n",
      "\n",
      "[50 rows x 775 columns]\n"
     ]
    }
   ],
   "source": [
    "print(X_group_1.head(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for group 1 (B1-B2): 0.8379310344827586\n"
     ]
    }
   ],
   "source": [
    "# Scale features for group 1\n",
    "X_group_1_scaled = scaler.transform(X_group_1)  # Use the same scaler as before\n",
    "\n",
    "# Train-test split for group 1\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_group_1_scaled, y_group_1, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train a new model for group 1\n",
    "model_group_1 = SVC(**grid_search.best_params_,random_state=42)\n",
    "model_group_1.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model for group 1\n",
    "y_pred_group_1 = model_group_1.predict(X_test)\n",
    "print(\"Accuracy for group 1 (B1-B2):\", accuracy_score(y_test, y_pred_group_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group 2 = C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data for group 2 (C1-C2)\n",
    "group_2_data = training_data[(group_predictions == 2) & (training_data['difficulty_encoded'].isin([5, 6]))]\n",
    "X_group_2 = group_2_data.drop(['id', 'sentence', 'difficulty', 'processed_sentence', 'difficulty_binary', 'difficulty_group', 'difficulty_encoded'], axis=1)\n",
    "y_group_2 = group_2_data['difficulty_encoded']  # 'difficulty_encoded' with values 5 and 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_group_2.head(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for group 2 (C1-C2): 0.7605177993527508\n"
     ]
    }
   ],
   "source": [
    "# Scale features for group 2\n",
    "X_group_2_scaled = scaler.transform(X_group_2)  # Use the same scaler as before\n",
    "\n",
    "# Train-test split for group 2\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_group_2_scaled, y_group_2, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train a new model for group 2\n",
    "model_group_2 = SVC(**grid_search.best_params_,random_state=42)\n",
    "model_group_2.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model for group 2\n",
    "y_pred_group_2 = model_group_2.predict(X_test)\n",
    "print(\"Accuracy for group 2 (C1-C2):\", accuracy_score(y_test, y_pred_group_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting on the unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test data\n",
    "test_data = pd.read_csv('Data/3model_test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id                                           sentence  \\\n",
      "0   0  Nous dûmes nous excuser des propos que nous eû...   \n",
      "1   1  Vous ne pouvez pas savoir le plaisir que j'ai ...   \n",
      "2   2  Et, paradoxalement, boire froid n'est pas la b...   \n",
      "3   3  Ce n'est pas étonnant, car c'est une saison my...   \n",
      "4   4  Le corps de Golo lui-même, d'une essence aussi...   \n",
      "\n",
      "                                  processed_sentence  LEN  UNIQUE_WORD_COUNT  \\\n",
      "0  nous dûmes nous excuser des propos que nous eû...   10                  8   \n",
      "1  vous ne pouvez pas savoir le plaisir que jai d...   14                 14   \n",
      "2  et paradoxalement boire froid nest pas la bonn...    9                  9   \n",
      "3  ce nest pas étonnant car cest une saison mysté...    9                  9   \n",
      "4  le corps de golo luimême dune essence aussi su...   72                 56   \n",
      "\n",
      "   AVG_WORD_LENGTH   DCRS   FKG   ARI  SYLLABLE_COUNT  ...  embedding_758  \\\n",
      "0         5.000000  16.76   4.8   7.1              15  ...      -0.091074   \n",
      "1         4.571429  20.12   4.0   7.1              20  ...      -0.048117   \n",
      "2         5.111111  16.36   3.3   7.1              14  ...      -0.022979   \n",
      "3         4.888889  16.36   3.3   6.1              13  ...      -0.002399   \n",
      "4         5.236111  18.83  30.2  39.3             122  ...       0.039166   \n",
      "\n",
      "   embedding_759  embedding_760  embedding_761  embedding_762  embedding_763  \\\n",
      "0      -0.057849      -0.153783       0.113769       0.082491       0.007048   \n",
      "1      -0.051733      -0.169284       0.072031       0.052745      -0.041019   \n",
      "2      -0.170090      -0.108290       0.151637       0.012537       0.052576   \n",
      "3      -0.119357      -0.086169       0.117364      -0.009256       0.032399   \n",
      "4      -0.021739      -0.130386      -0.106114       0.072774       0.005718   \n",
      "\n",
      "   embedding_764  embedding_765  embedding_766  embedding_767  \n",
      "0       0.068648      -0.106739       0.018842       0.003663  \n",
      "1       0.026564      -0.157390       0.003122       0.005944  \n",
      "2       0.072825      -0.054117      -0.000034      -0.021972  \n",
      "3       0.088059      -0.103890      -0.026876      -0.039505  \n",
      "4       0.045151      -0.037816       0.043461      -0.021040  \n",
      "\n",
      "[5 rows x 778 columns]\n"
     ]
    }
   ],
   "source": [
    "print(test_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = test_data.drop(['id','sentence', 'processed_sentence'], axis=1)\n",
    "\n",
    "# Scale the features\n",
    "X_new_scaled = scaler.transform(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Group Classification and Exact CEFR Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict group classification\n",
    "group_predictions_new = final_model.predict(X_new_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example for group 0 (A1-A2)\n",
    "group_0_indices = group_predictions_new == 0\n",
    "X_group_0_new = X_new_scaled[group_0_indices]\n",
    "predicted_cefr_group_0 = model_group_0.predict(X_group_0_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict exact CEFR level within Group 1\n",
    "group_1_indices = group_predictions_new == 1\n",
    "X_group_1_new = X_new_scaled[group_1_indices]\n",
    "predicted_cefr_group_1 = model_group_1.predict(X_group_1_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict exact CEFR level within Group 2\n",
    "group_2_indices = group_predictions_new == 2\n",
    "X_group_2_new = X_new_scaled[group_2_indices]\n",
    "predicted_cefr_group_2 = model_group_2.predict(X_group_2_new)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine the predictions again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an array to store all predictions\n",
    "combined_predictions = np.empty(len(test_data), dtype=object)\n",
    "\n",
    "# Assign predictions from each group to the combined array\n",
    "combined_predictions[group_0_indices] = predicted_cefr_group_0\n",
    "combined_predictions[group_1_indices] = predicted_cefr_group_1\n",
    "combined_predictions[group_2_indices] = predicted_cefr_group_2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reverse encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_numeric_to_cefr(numeric_value):\n",
    "    cefr_mapping = {1: 'A1', 2: 'A2', 3: 'B1', 4: 'B2', 5: 'C1', 6: 'C2'}\n",
    "    return cefr_mapping.get(numeric_value, \"Unknown\")\n",
    "\n",
    "# Apply the mapping to the predictions of each group\n",
    "predicted_cefr_group_0_mapped = [map_numeric_to_cefr(num) for num in predicted_cefr_group_0]\n",
    "predicted_cefr_group_1_mapped = [map_numeric_to_cefr(num) for num in predicted_cefr_group_1]\n",
    "predicted_cefr_group_2_mapped = [map_numeric_to_cefr(num) for num in predicted_cefr_group_2]\n",
    "\n",
    "# Initialize an array to store all mapped predictions\n",
    "combined_predictions_mapped = np.empty(len(test_data), dtype=object)\n",
    "\n",
    "# Assign mapped predictions from each group to the combined array\n",
    "combined_predictions_mapped[group_0_indices] = predicted_cefr_group_0_mapped\n",
    "combined_predictions_mapped[group_1_indices] = predicted_cefr_group_1_mapped\n",
    "combined_predictions_mapped[group_2_indices] = predicted_cefr_group_2_mapped\n",
    "\n",
    "# Add predicted CEFR level to the original data\n",
    "test_data['difficulty'] = combined_predictions_mapped\n",
    "\n",
    "# Output the final DataFrame with the original order of sentences and predicted CEFR levels\n",
    "final_output = test_data[['sentence', 'difficulty']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            sentence difficulty\n",
      "0  Nous dûmes nous excuser des propos que nous eû...         C2\n",
      "1  Vous ne pouvez pas savoir le plaisir que j'ai ...         B1\n",
      "2  Et, paradoxalement, boire froid n'est pas la b...         B1\n",
      "3  Ce n'est pas étonnant, car c'est une saison my...         A1\n",
      "4  Le corps de Golo lui-même, d'une essence aussi...         C2\n"
     ]
    }
   ],
   "source": [
    "print(final_output.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[['id', 'difficulty']].to_csv('Data/3model_Nvidia_submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
