{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path_training = \"Data/training_data.csv\"\n",
    "file_path_test = \"Data/unlabelled_test_data.csv\"\n",
    "training_data = pd.read_csv(file_path_training)\n",
    "test_data = pd.read_csv(file_path_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few rows of the DataFrame:\n",
      "    id                                           sentence difficulty\n",
      "0    0  Les coûts kilométriques réels peuvent diverger...         C1\n",
      "1    1  Le bleu, c'est ma couleur préférée mais je n'a...         A1\n",
      "2    2  Le test de niveau en français est sur le site ...         A1\n",
      "3    3           Est-ce que ton mari est aussi de Boston?         A1\n",
      "4    4  Dans les écoles de commerce, dans les couloirs...         B1\n",
      "5    5  voilà une autre histoire que j'ai beaucoup aimée.         A2\n",
      "6    6  Les médecins disent souvent qu'on doit boire u...         A2\n",
      "7    7  Il est particulièrement observé chez les perso...         B2\n",
      "8    8  J'ai retrouvé le plaisir de manger un oeuf à l...         A2\n",
      "9    9  Nous allons bien, nous habitons dans une petit...         B1\n",
      "10  10                            Bonjour et bonne année.         A1\n",
      "11  11  La presse s'est abondamment fait l'écho des ef...         B2\n",
      "12  12  Pour que le rocher s'ouvre, il faut le toucher...         B1\n",
      "13  13  J'habite une belle ville dans le nord de la Fr...         A1\n",
      "14  14  Certes il doit répondre aux goûts du consommat...         B2\n",
      "15  15  Ma timidité me quittait dès que je m'éloignais...         C2\n",
      "16  16  Je pense que fréquenter les galeries est la me...         B2\n",
      "17  17  Le soir, je me cuche tôt parce que je dois êtr...         A1\n",
      "18  18  La mère de Raphaël et de Vanessa exprime sa sa...         B2\n",
      "19  19            Je ne fais pas grand-chose à la maison.         A2\n",
      "   id                                           sentence\n",
      "0   0  Nous dûmes nous excuser des propos que nous eû...\n",
      "1   1  Vous ne pouvez pas savoir le plaisir que j'ai ...\n",
      "2   2  Et, paradoxalement, boire froid n'est pas la b...\n",
      "3   3  Ce n'est pas étonnant, car c'est une saison my...\n",
      "4   4  Le corps de Golo lui-même, d'une essence aussi...\n"
     ]
    }
   ],
   "source": [
    "# Display the first few rows of the DataFrame\n",
    "print(\"First few rows of the DataFrame:\")\n",
    "print(training_data.head(20))\n",
    "print(test_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id                                           sentence difficulty  \\\n",
      "0   0  Les coûts kilométriques réels peuvent diverger...         C1   \n",
      "1   1  Le bleu, c'est ma couleur préférée mais je n'a...         A1   \n",
      "2   2  Le test de niveau en français est sur le site ...         A1   \n",
      "3   3           Est-ce que ton mari est aussi de Boston?         A1   \n",
      "4   4  Dans les écoles de commerce, dans les couloirs...         B1   \n",
      "\n",
      "                                  processed_sentence  \n",
      "0  les coûts kilométriques réels peuvent diverger...  \n",
      "1  le bleu cest ma couleur préférée mais je naime...  \n",
      "2  le test de niveau en français est sur le site ...  \n",
      "3             estce que ton mari est aussi de boston  \n",
      "4  dans les écoles de commerce dans les couloirs ...  \n",
      "   id                                           sentence  \\\n",
      "0   0  Nous dûmes nous excuser des propos que nous eû...   \n",
      "1   1  Vous ne pouvez pas savoir le plaisir que j'ai ...   \n",
      "2   2  Et, paradoxalement, boire froid n'est pas la b...   \n",
      "3   3  Ce n'est pas étonnant, car c'est une saison my...   \n",
      "4   4  Le corps de Golo lui-même, d'une essence aussi...   \n",
      "\n",
      "                                  processed_sentence  \n",
      "0  nous dûmes nous excuser des propos que nous eû...  \n",
      "1  vous ne pouvez pas savoir le plaisir que jai d...  \n",
      "2  et paradoxalement boire froid nest pas la bonn...  \n",
      "3  ce nest pas étonnant car cest une saison mysté...  \n",
      "4  le corps de golo luimême dune essence aussi su...  \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_text(sentence):\n",
    "    # Convert to lowercase\n",
    "    sentence = sentence.lower()\n",
    "    # Remove unnecessary punctuation while retaining French accents and special characters\n",
    "    sentence = re.sub(r'[^a-zàâçéèêëîïôûùüÿñæœ\\s]', '', sentence)\n",
    "    return sentence\n",
    "\n",
    "# Apply the preprocessing to each sentence\n",
    "training_data['processed_sentence'] = training_data['sentence'].apply(preprocess_text)\n",
    "test_data['processed_sentence'] = test_data['sentence'].apply(preprocess_text)\n",
    "# Display the first few rows of the dataframe after preprocessing\n",
    "print(training_data.head())\n",
    "print(test_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CamembertTokenizer, CamembertModel\n",
    "import torch\n",
    "import textstat\n",
    "import pyphen\n",
    "\n",
    "# Initialize CamemBERT tokenizer and model\n",
    "tokenizer = CamembertTokenizer.from_pretrained('camembert-base')\n",
    "model = CamembertModel.from_pretrained('camembert-base')\n",
    "# Initialize Pyphen for syllable counting\n",
    "dic = pyphen.Pyphen(lang='fr')\n",
    "\n",
    "def get_camembert_embedding(sentence):\n",
    "    # Tokenize and encode the sentence for CamemBERT\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    # Get the embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # Extract the embeddings for the [CLS] token (representing the entire sentence)\n",
    "    embeddings = outputs.last_hidden_state[:, 0, :].squeeze().numpy()\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def count_syllables(word):\n",
    "    # Count the hyphens as an approximation of syllable count\n",
    "    hyphenated = dic.inserted(word)\n",
    "    return hyphenated.count('-') + 1\n",
    "\n",
    "\n",
    "# Apply feature engineering and concatenate CamemBERT embeddings\n",
    "def add_features(df):\n",
    "    # Adding features based on text properties\n",
    "    df['LEN'] = df['processed_sentence'].apply(lambda x: len(x.split()))\n",
    "    df['UNIQUE_WORD_COUNT'] = df['processed_sentence'].apply(lambda x: len(set(x.split())))\n",
    "    df['AVG_WORD_LENGTH'] = df['processed_sentence'].apply(lambda x: sum(len(word) for word in x.split()) / len(x.split()) if x.split() else 0)\n",
    "    df['DCRS'] = df['processed_sentence'].apply(lambda x: textstat.dale_chall_readability_score(x))\n",
    "    df['FKG'] = df['processed_sentence'].apply(lambda x: textstat.flesch_kincaid_grade(x))\n",
    "    df['ARI'] = df['processed_sentence'].apply(lambda x: textstat.automated_readability_index(x))\n",
    "    df['SYLLABLE_COUNT'] = df['processed_sentence'].apply(lambda x: sum(count_syllables(word) for word in x.split()))\n",
    "\n",
    "    # Adding CamemBERT embeddings\n",
    "    embeddings = df['processed_sentence'].apply(get_camembert_embedding).tolist()\n",
    "    # Generate column names for embeddings\n",
    "    embedding_column_names = [f'embedding_{i}' for i in range(len(embeddings[0]))]\n",
    "    embeddings_df = pd.DataFrame(embeddings, columns=embedding_column_names)\n",
    "    df = pd.concat([df, embeddings_df], axis=1)\n",
    "\n",
    "    return df\n",
    "# Apply feature engineering\n",
    "training_data = add_features(training_data)\n",
    "test_data = add_features(test_data)\n",
    "\n",
    "# Encoding the difficulty levels\n",
    "difficulty_encoding = {'A1': 1, 'A2': 2, 'B1': 3, 'B2': 4, 'C1': 5, 'C2': 6}\n",
    "training_data['difficulty_encoded'] = training_data['difficulty'].map(difficulty_encoding)\n",
    "\n",
    "# Display the dataframe with new features\n",
    "print(training_data.head())\n",
    "print(test_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add binairy difficulty column\n",
    "This way the difficulty is first divided in easy and difficult and later the CEFR label is given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_difficulty_to_binary(difficulty):\n",
    "    # Mapping lower difficulties (A1 to B1) to 0 and higher difficulties (B2 to C2) to 1\n",
    "    return 0 if difficulty in ['A1', 'A2', 'B1'] else 1\n",
    "\n",
    "# Apply the mapping to create a new boolean column\n",
    "training_data['difficulty_binary'] = training_data['difficulty'].apply(map_difficulty_to_binary)\n",
    "\n",
    "# Display the dataframe with the new column\n",
    "training_data.head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create another column to split the difficulty per letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>difficulty</th>\n",
       "      <th>processed_sentence</th>\n",
       "      <th>LEN</th>\n",
       "      <th>UNIQUE_WORD_COUNT</th>\n",
       "      <th>AVG_WORD_LENGTH</th>\n",
       "      <th>SYLLABLE_COUNT</th>\n",
       "      <th>FKG</th>\n",
       "      <th>ARI</th>\n",
       "      <th>...</th>\n",
       "      <th>embedding_759</th>\n",
       "      <th>embedding_760</th>\n",
       "      <th>embedding_761</th>\n",
       "      <th>embedding_762</th>\n",
       "      <th>embedding_763</th>\n",
       "      <th>embedding_764</th>\n",
       "      <th>embedding_765</th>\n",
       "      <th>embedding_766</th>\n",
       "      <th>embedding_767</th>\n",
       "      <th>difficulty_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Les coûts kilométriques réels peuvent diverger...</td>\n",
       "      <td>C1</td>\n",
       "      <td>les coûts kilométriques réels peuvent diverger...</td>\n",
       "      <td>38</td>\n",
       "      <td>29</td>\n",
       "      <td>5.526316</td>\n",
       "      <td>65</td>\n",
       "      <td>18.1</td>\n",
       "      <td>23.6</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.067223</td>\n",
       "      <td>-0.065459</td>\n",
       "      <td>0.134300</td>\n",
       "      <td>-0.011380</td>\n",
       "      <td>0.022304</td>\n",
       "      <td>0.008905</td>\n",
       "      <td>-0.214924</td>\n",
       "      <td>0.004331</td>\n",
       "      <td>-0.054215</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Le bleu, c'est ma couleur préférée mais je n'a...</td>\n",
       "      <td>A1</td>\n",
       "      <td>le bleu cest ma couleur préférée mais je naime...</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>3.916667</td>\n",
       "      <td>15</td>\n",
       "      <td>0.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.152003</td>\n",
       "      <td>-0.081141</td>\n",
       "      <td>0.126171</td>\n",
       "      <td>0.053080</td>\n",
       "      <td>0.040183</td>\n",
       "      <td>0.092430</td>\n",
       "      <td>-0.128848</td>\n",
       "      <td>-0.034450</td>\n",
       "      <td>0.043938</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Le test de niveau en français est sur le site ...</td>\n",
       "      <td>A1</td>\n",
       "      <td>le test de niveau en français est sur le site ...</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>18</td>\n",
       "      <td>3.6</td>\n",
       "      <td>3.9</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.074644</td>\n",
       "      <td>-0.177957</td>\n",
       "      <td>0.087216</td>\n",
       "      <td>0.031253</td>\n",
       "      <td>0.014485</td>\n",
       "      <td>0.073736</td>\n",
       "      <td>-0.044085</td>\n",
       "      <td>0.000471</td>\n",
       "      <td>-0.027738</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Est-ce que ton mari est aussi de Boston?</td>\n",
       "      <td>A1</td>\n",
       "      <td>estce que ton mari est aussi de boston</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>3.875000</td>\n",
       "      <td>11</td>\n",
       "      <td>2.9</td>\n",
       "      <td>0.8</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.096884</td>\n",
       "      <td>-0.183525</td>\n",
       "      <td>0.121947</td>\n",
       "      <td>-0.008364</td>\n",
       "      <td>0.084223</td>\n",
       "      <td>0.092719</td>\n",
       "      <td>-0.132519</td>\n",
       "      <td>0.058858</td>\n",
       "      <td>0.007657</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Dans les écoles de commerce, dans les couloirs...</td>\n",
       "      <td>B1</td>\n",
       "      <td>dans les écoles de commerce dans les couloirs ...</td>\n",
       "      <td>32</td>\n",
       "      <td>24</td>\n",
       "      <td>5.093750</td>\n",
       "      <td>45</td>\n",
       "      <td>13.4</td>\n",
       "      <td>18.5</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.074465</td>\n",
       "      <td>-0.070021</td>\n",
       "      <td>0.052438</td>\n",
       "      <td>0.013443</td>\n",
       "      <td>0.033107</td>\n",
       "      <td>0.002078</td>\n",
       "      <td>-0.140131</td>\n",
       "      <td>0.009309</td>\n",
       "      <td>-0.019106</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1550 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                           sentence difficulty  \\\n",
       "0   0  Les coûts kilométriques réels peuvent diverger...         C1   \n",
       "1   1  Le bleu, c'est ma couleur préférée mais je n'a...         A1   \n",
       "2   2  Le test de niveau en français est sur le site ...         A1   \n",
       "3   3           Est-ce que ton mari est aussi de Boston?         A1   \n",
       "4   4  Dans les écoles de commerce, dans les couloirs...         B1   \n",
       "\n",
       "                                  processed_sentence  LEN  UNIQUE_WORD_COUNT  \\\n",
       "0  les coûts kilométriques réels peuvent diverger...   38                 29   \n",
       "1  le bleu cest ma couleur préférée mais je naime...   12                 11   \n",
       "2  le test de niveau en français est sur le site ...   13                 11   \n",
       "3             estce que ton mari est aussi de boston    8                  8   \n",
       "4  dans les écoles de commerce dans les couloirs ...   32                 24   \n",
       "\n",
       "   AVG_WORD_LENGTH  SYLLABLE_COUNT   FKG   ARI  ...  embedding_759  \\\n",
       "0         5.526316              65  18.1  23.6  ...      -0.067223   \n",
       "1         3.916667              15   0.9   3.0  ...      -0.152003   \n",
       "2         4.000000              18   3.6   3.9  ...      -0.074644   \n",
       "3         3.875000              11   2.9   0.8  ...      -0.096884   \n",
       "4         5.093750              45  13.4  18.5  ...      -0.074465   \n",
       "\n",
       "   embedding_760  embedding_761  embedding_762  embedding_763  embedding_764  \\\n",
       "0      -0.065459       0.134300      -0.011380       0.022304       0.008905   \n",
       "1      -0.081141       0.126171       0.053080       0.040183       0.092430   \n",
       "2      -0.177957       0.087216       0.031253       0.014485       0.073736   \n",
       "3      -0.183525       0.121947      -0.008364       0.084223       0.092719   \n",
       "4      -0.070021       0.052438       0.013443       0.033107       0.002078   \n",
       "\n",
       "   embedding_765  embedding_766  embedding_767  difficulty_group  \n",
       "0      -0.214924       0.004331      -0.054215                 2  \n",
       "1      -0.128848      -0.034450       0.043938                 0  \n",
       "2      -0.044085       0.000471      -0.027738                 0  \n",
       "3      -0.132519       0.058858       0.007657                 0  \n",
       "4      -0.140131       0.009309      -0.019106                 1  \n",
       "\n",
       "[5 rows x 1550 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def map_difficulty_to_group(difficulty):\n",
    "    if difficulty in ['A1', 'A2']:\n",
    "        return 0\n",
    "    elif difficulty in ['B1', 'B2']:\n",
    "        return 1\n",
    "    else: # Assuming remaining are 'C1' and 'C2'\n",
    "        return 2\n",
    "\n",
    "# Apply the mapping to create the new column\n",
    "training_data['difficulty_group'] = training_data['difficulty'].apply(map_difficulty_to_group)\n",
    "\n",
    "# Display the first few rows to verify the new column\n",
    "training_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the data in a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "training_data.to_csv('Data/3model_training_data.csv', index= False)\n",
    "test_data.to_csv('Data/3model_test_data.csv', index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.read_csv('Data/3model_training_data.csv')\n",
    "test_data = pd.read_csv('Data/3model_test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(training_data.head())\n",
    "print(test_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for the first training\n",
    "Now we going to train a model that can classify the training in 2 categories: easy and hard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Scores: [0.73854167 0.74166667 0.74583333 0.75208333 0.740625  ]\n",
      "Average CV Score: 0.74375\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "\n",
    "# Extract features and labels\n",
    "X = training_data.drop(['id', 'sentence', 'difficulty', 'processed_sentence', 'difficulty_encoded', 'difficulty_binary', 'difficulty_group'], axis=1)\n",
    "y = training_data['difficulty_group']\n",
    "\n",
    "# Ensure all column names are of type string\n",
    "X.columns = X.columns.astype(str)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Scores: [0.75625    0.76666667 0.74375    0.74479167 0.73958333]\n",
      "Average CV Score: 0.7502083333333334\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Initialize the SVM classifier\n",
    "model = SVC(random_state=42)\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores = cross_val_score(model, X_scaled, y, cv=5)\n",
    "\n",
    "# Print the cross-validation results\n",
    "print(\"Cross-Validation Scores:\", cv_scores)\n",
    "print(\"Average CV Score:\", cv_scores.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "[CV] END ..................C=0.1, gamma=scale, kernel=linear; total time=  16.6s\n",
      "[CV] END ..................C=0.1, gamma=scale, kernel=linear; total time=  15.3s\n",
      "[CV] END ..................C=0.1, gamma=scale, kernel=linear; total time=  17.0s\n",
      "[CV] END ..................C=0.1, gamma=scale, kernel=linear; total time=  16.2s\n",
      "[CV] END ..................C=0.1, gamma=scale, kernel=linear; total time=  15.8s\n",
      "[CV] END .....................C=0.1, gamma=scale, kernel=rbf; total time=  17.5s\n",
      "[CV] END .....................C=0.1, gamma=scale, kernel=rbf; total time=  17.3s\n",
      "[CV] END .....................C=0.1, gamma=scale, kernel=rbf; total time=  17.2s\n",
      "[CV] END .....................C=0.1, gamma=scale, kernel=rbf; total time=  17.4s\n",
      "[CV] END .....................C=0.1, gamma=scale, kernel=rbf; total time=  17.4s\n",
      "[CV] END ....................C=0.1, gamma=scale, kernel=poly; total time=  20.7s\n",
      "[CV] END ....................C=0.1, gamma=scale, kernel=poly; total time=  19.8s\n",
      "[CV] END ....................C=0.1, gamma=scale, kernel=poly; total time=  20.5s\n",
      "[CV] END ....................C=0.1, gamma=scale, kernel=poly; total time=  20.6s\n",
      "[CV] END ....................C=0.1, gamma=scale, kernel=poly; total time=  22.0s\n",
      "[CV] END ...................C=0.1, gamma=auto, kernel=linear; total time=  17.8s\n",
      "[CV] END ...................C=0.1, gamma=auto, kernel=linear; total time=  15.3s\n",
      "[CV] END ...................C=0.1, gamma=auto, kernel=linear; total time=  16.7s\n",
      "[CV] END ...................C=0.1, gamma=auto, kernel=linear; total time=  16.1s\n",
      "[CV] END ...................C=0.1, gamma=auto, kernel=linear; total time=  15.8s\n",
      "[CV] END ......................C=0.1, gamma=auto, kernel=rbf; total time=  17.3s\n",
      "[CV] END ......................C=0.1, gamma=auto, kernel=rbf; total time=  17.0s\n",
      "[CV] END ......................C=0.1, gamma=auto, kernel=rbf; total time=  16.9s\n",
      "[CV] END ......................C=0.1, gamma=auto, kernel=rbf; total time=  16.9s\n",
      "[CV] END ......................C=0.1, gamma=auto, kernel=rbf; total time=  17.2s\n",
      "[CV] END .....................C=0.1, gamma=auto, kernel=poly; total time=  20.2s\n",
      "[CV] END .....................C=0.1, gamma=auto, kernel=poly; total time=  20.1s\n",
      "[CV] END .....................C=0.1, gamma=auto, kernel=poly; total time=  20.3s\n",
      "[CV] END .....................C=0.1, gamma=auto, kernel=poly; total time=  20.1s\n",
      "[CV] END .....................C=0.1, gamma=auto, kernel=poly; total time=  20.0s\n",
      "[CV] END ....................C=1, gamma=scale, kernel=linear; total time= 1.1min\n",
      "[CV] END ....................C=1, gamma=scale, kernel=linear; total time= 1.4min\n",
      "[CV] END ....................C=1, gamma=scale, kernel=linear; total time= 1.5min\n",
      "[CV] END ....................C=1, gamma=scale, kernel=linear; total time= 1.4min\n",
      "[CV] END ....................C=1, gamma=scale, kernel=linear; total time= 1.2min\n",
      "[CV] END .......................C=1, gamma=scale, kernel=rbf; total time=  14.5s\n",
      "[CV] END .......................C=1, gamma=scale, kernel=rbf; total time=  14.5s\n",
      "[CV] END .......................C=1, gamma=scale, kernel=rbf; total time=  14.2s\n",
      "[CV] END .......................C=1, gamma=scale, kernel=rbf; total time=  14.2s\n",
      "[CV] END .......................C=1, gamma=scale, kernel=rbf; total time=  14.1s\n",
      "[CV] END ......................C=1, gamma=scale, kernel=poly; total time=  17.6s\n",
      "[CV] END ......................C=1, gamma=scale, kernel=poly; total time=  17.2s\n",
      "[CV] END ......................C=1, gamma=scale, kernel=poly; total time=  17.5s\n",
      "[CV] END ......................C=1, gamma=scale, kernel=poly; total time=  17.5s\n",
      "[CV] END ......................C=1, gamma=scale, kernel=poly; total time=  17.4s\n",
      "[CV] END .....................C=1, gamma=auto, kernel=linear; total time= 1.2min\n",
      "[CV] END .....................C=1, gamma=auto, kernel=linear; total time= 1.4min\n",
      "[CV] END .....................C=1, gamma=auto, kernel=linear; total time= 1.4min\n",
      "[CV] END .....................C=1, gamma=auto, kernel=linear; total time= 1.4min\n",
      "[CV] END .....................C=1, gamma=auto, kernel=linear; total time= 1.2min\n",
      "[CV] END ........................C=1, gamma=auto, kernel=rbf; total time=  14.2s\n",
      "[CV] END ........................C=1, gamma=auto, kernel=rbf; total time=  14.1s\n",
      "[CV] END ........................C=1, gamma=auto, kernel=rbf; total time=  13.9s\n",
      "[CV] END ........................C=1, gamma=auto, kernel=rbf; total time=  14.3s\n",
      "[CV] END ........................C=1, gamma=auto, kernel=rbf; total time=  14.1s\n",
      "[CV] END .......................C=1, gamma=auto, kernel=poly; total time=  17.4s\n",
      "[CV] END .......................C=1, gamma=auto, kernel=poly; total time=  17.1s\n",
      "[CV] END .......................C=1, gamma=auto, kernel=poly; total time=  17.0s\n",
      "[CV] END .......................C=1, gamma=auto, kernel=poly; total time=  17.2s\n",
      "[CV] END .......................C=1, gamma=auto, kernel=poly; total time=  16.9s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10], \n",
    "    'gamma': ['scale', 'auto'], \n",
    "    'kernel': ['rbf', 'poly']\n",
    "}\n",
    "\n",
    "# Create a GridSearchCV object\n",
    "grid_search = GridSearchCV(SVC(random_state=42), param_grid, cv=5, scoring='accuracy', verbose=2)\n",
    "\n",
    "# Perform grid search\n",
    "grid_search.fit(X_scaled, y)\n",
    "\n",
    "# Print the best parameters and the corresponding score\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation score:\", grid_search.best_score_)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
