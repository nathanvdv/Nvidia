{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>difficulty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Les coûts kilométriques réels peuvent diverger...</td>\n",
       "      <td>C1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Le bleu, c'est ma couleur préférée mais je n'a...</td>\n",
       "      <td>A1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Le test de niveau en français est sur le site ...</td>\n",
       "      <td>A1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Est-ce que ton mari est aussi de Boston?</td>\n",
       "      <td>A1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Dans les écoles de commerce, dans les couloirs...</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                           sentence difficulty\n",
       "0   0  Les coûts kilométriques réels peuvent diverger...         C1\n",
       "1   1  Le bleu, c'est ma couleur préférée mais je n'a...         A1\n",
       "2   2  Le test de niveau en français est sur le site ...         A1\n",
       "3   3           Est-ce que ton mari est aussi de Boston?         A1\n",
       "4   4  Dans les écoles de commerce, dans les couloirs...         B1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Reload the dataset\n",
    "training_data = pd.read_csv('/home/nathan/OneDrive/GitHub/Nvidia/Data/training_data.csv')\n",
    "\n",
    "# Check for missing values and basic statistics\n",
    "training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import spacy\n",
    "\n",
    "# Load the French language model for spaCy\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "# Text Cleaning Function\n",
    "def clean_text(text):\n",
    "    # Lowercasing the text\n",
    "    text = text.lower()\n",
    "    # Removing punctuations\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Removing numbers and other non-letter characters\n",
    "    text = re.sub(r'[^a-zàâçéèêëîïôûùüÿñæœ]', ' ', text)\n",
    "    return text\n",
    "\n",
    "# POS Tagging Function\n",
    "def pos_tagging(text):\n",
    "    doc = nlp(text)\n",
    "    pos_tags = [token.pos_ for token in doc]\n",
    "    return pos_tags\n",
    "\n",
    "# Applying the cleaning function to the dataset\n",
    "training_data['cleaned_sentence'] = training_data['sentence'].apply(clean_text)\n",
    "\n",
    "# Feature Engineering\n",
    "# Adding sentence length and word count\n",
    "training_data['sentence_length'] = training_data['cleaned_sentence'].apply(len)\n",
    "training_data['word_count'] = training_data['cleaned_sentence'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Adding POS tagging\n",
    "training_data['pos_tags'] = training_data['cleaned_sentence'].apply(pos_tagging)\n",
    "\n",
    "# Displaying the first few rows of the updated dataset\n",
    "training_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Statistical Analysis of Sentence Features\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Grouping by difficulty level and calculating mean and standard deviation\n",
    "grouped_data = training_data.groupby('difficulty').agg({'sentence_length': ['mean', 'std'], 'word_count': ['mean', 'std']}).reset_index()\n",
    "\n",
    "# Plotting the sentence length and word count for each difficulty level\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot for Sentence Length\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.barplot(x='difficulty', y=('sentence_length', 'mean'), data=grouped_data)\n",
    "plt.title('Average Sentence Length by Difficulty Level')\n",
    "\n",
    "# Plot for Word Count\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.barplot(x='difficulty', y=('word_count', 'mean'), data=grouped_data)\n",
    "plt.title('Average Word Count by Difficulty Level')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the difficulty levels\n",
    "from sklearn.calibration import LabelEncoder\n",
    "\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "training_data['difficulty_encoded'] = label_encoder.fit_transform(training_data['difficulty'])\n",
    "\n",
    "# Display the first few rows of the modified dataframe\n",
    "df_encoded_head = training_data.head()\n",
    "encoded_classes = label_encoder.classes_\n",
    "\n",
    "df_encoded_head, encoded_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12 min +-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CamembertTokenizer, CamembertModel\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the tokenizer and model\n",
    "tokenizer = CamembertTokenizer.from_pretrained(\"camembert-base\")\n",
    "model = CamembertModel.from_pretrained(\"camembert-base\")\n",
    "\n",
    "# Function to encode sentences in batches\n",
    "def encode_sentences_in_batches(sentences, batch_size=64):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    batched_embeddings = []\n",
    "\n",
    "    for i in range(0, len(sentences), batch_size):\n",
    "        batch_sentences = sentences[i:i + batch_size]\n",
    "        inputs = tokenizer(batch_sentences, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state[:, 0, :].numpy()\n",
    "        batched_embeddings.append(embeddings)\n",
    "\n",
    "    return np.vstack(batched_embeddings)\n",
    "\n",
    "# Tokenize and encode sentences in batches\n",
    "encoded_sentences = encode_sentences_in_batches(training_data['sentence'].tolist())\n",
    "\n",
    "# Use encoded_sentences for training the logistic regression model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(encoded_sentences)\n",
    "np.save('/home/nathan/OneDrive/GitHub/Nvidia/Data/encoded_sentences.npy', encoded_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Since we can't extract embeddings here, we'll assume `encoded_sentences` is available\n",
    "# Let's pretend we have a variable encoded_sentences which contains the embeddings\n",
    "encoded_sentences = np.load('/home/nathan/OneDrive/GitHub/Nvidia/Data/encoded_sentences.npy')\n",
    "# Train-test split\n",
    "X_train, X_val, y_train, y_val = train_test_split(encoded_sentences, training_data['difficulty_encoded'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logistic_regression_model = LogisticRegression(max_iter=1000)\n",
    "logistic_regression_model.fit(X_train, y_train)\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn_model = KNeighborsClassifier(n_neighbors=11)  # You can tune 'n_neighbors'\n",
    "knn_model.fit(X_train, y_train)\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "decision_tree_model = DecisionTreeClassifier()\n",
    "decision_tree_model.fit(X_train, y_train)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "random_forest_model = RandomForestClassifier(n_estimators=100)\n",
    "random_forest_model.fit(X_train, y_train)\n",
    "from sklearn.svm import SVC\n",
    "svm_model = SVC(kernel='linear')\n",
    "svm_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "\n",
    "# Logistic Regression\n",
    "lr_predictions = logistic_regression_model.predict(X_val)\n",
    "print(\"Logistic Regression:\")\n",
    "print(classification_report(y_val, lr_predictions))\n",
    "print(\"Accuracy:\", accuracy_score(y_val, lr_predictions))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_val, lr_predictions))\n",
    "\n",
    "# k-Nearest Neighbors\n",
    "knn_predictions = knn_model.predict(X_val)\n",
    "print(\"\\nk-Nearest Neighbors:\")\n",
    "print(classification_report(y_val, knn_predictions))\n",
    "print(\"Accuracy:\", accuracy_score(y_val, knn_predictions))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_val, knn_predictions))\n",
    "\n",
    "# Decision Tree\n",
    "dt_predictions = decision_tree_model.predict(X_val)\n",
    "print(\"\\nDecision Tree:\")\n",
    "print(classification_report(y_val, dt_predictions))\n",
    "print(\"Accuracy:\", accuracy_score(y_val, dt_predictions))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_val, dt_predictions))\n",
    "\n",
    "# Random Forest\n",
    "rf_predictions = random_forest_model.predict(X_val)\n",
    "print(\"\\nRandom Forest:\")\n",
    "print(classification_report(y_val, rf_predictions))\n",
    "print(\"Accuracy:\", accuracy_score(y_val, rf_predictions))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_val, rf_predictions))\n",
    "\n",
    "# Support Vector Machine\n",
    "svm_predictions = svm_model.predict(X_val)\n",
    "print(\"\\nSupport Vector Machine:\")\n",
    "print(classification_report(y_val, svm_predictions))\n",
    "print(\"Accuracy:\", accuracy_score(y_val, svm_predictions))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_val, svm_predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained CamemBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import CamembertTokenizer, CamembertForSequenceClassification, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "# Custom Dataset Class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = CamembertTokenizer.from_pretrained(\"camembert-base\")\n",
    "\n",
    "# Prepare the dataset\n",
    "train_dataset = CustomDataset(\n",
    "    texts=training_data['sentence'].to_numpy(),\n",
    "    labels=training_data['difficulty_encoded'].to_numpy(),\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=128  # or any other value that suits your needs\n",
    ")\n",
    "\n",
    "# Model for Sequence Classification\n",
    "model = CamembertForSequenceClassification.from_pretrained(\"camembert-base\", num_labels=len(label_encoder.classes_))\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=4,\n",
    "    per_device_train_batch_size=32,\n",
    "    learning_rate=2e-5,\n",
    "    logging_dir='./logs',\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type='linear',  # Updated value\n",
    "    warmup_steps=500,\n",
    "    gradient_accumulation_steps=2,\n",
    "    max_grad_norm=1.0,\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"./my_finetuned_camembert\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import CamembertTokenizer, CamembertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Load the fine-tuned model\n",
    "model = CamembertForSequenceClassification.from_pretrained(\"./my_finetuned_camembert\")\n",
    "tokenizer = CamembertTokenizer.from_pretrained(\"camembert-base\")\n",
    "\n",
    "# Load and preprocess the unlabelled test data\n",
    "unlabelled_test_data = pd.read_csv('/home/nathan/OneDrive/GitHub/Nvidia/Data/unlabelled_test_data.csv')\n",
    "unlabelled_test_data['cleaned_sentence'] = unlabelled_test_data['sentence'].apply(clean_text)  # Use your clean_text function\n",
    "\n",
    "# Function to encode and predict labels for sentences\n",
    "def predict_labels(sentences, model, tokenizer, batch_size=64):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    predictions = []\n",
    "\n",
    "    for i in range(0, len(sentences), batch_size):\n",
    "        batch_sentences = sentences[i:i + batch_size]\n",
    "        inputs = tokenizer(batch_sentences, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        batch_predictions = torch.argmax(logits, dim=1).numpy()\n",
    "        predictions.extend(batch_predictions)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "# Predict labels for the unlabelled test data\n",
    "predicted_labels = predict_labels(unlabelled_test_data['cleaned_sentence'].tolist(), model, tokenizer)\n",
    "\n",
    "# Convert numerical labels back to original class names\n",
    "predicted_classes = label_encoder.inverse_transform(predicted_labels)\n",
    "\n",
    "# Adding predictions to the DataFrame\n",
    "unlabelled_test_data['difficulty'] = predicted_classes\n",
    "\n",
    "unlabelled_test_data[['id', 'difficulty']].to_csv('/home/nathan/OneDrive/GitHub/Nvidia/Data/Nvidia_CamemBERT_Enhanched_submission.csv', index=False)\n",
    "\n",
    "# Displaying the predictions\n",
    "unlabelled_test_data.head()\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Parameter grid\n",
    "param_grid_lr = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'solver': ['lbfgs', 'liblinear']\n",
    "}\n",
    "\n",
    "# Model\n",
    "logistic_regression_model = LogisticRegression(max_iter=10)\n",
    "grid_search_lr = GridSearchCV(logistic_regression_model, param_grid_lr, cv=5, scoring='accuracy')\n",
    "grid_search_lr.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters\n",
    "print(\"Best parameters for Logistic Regression:\", grid_search_lr.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Parameter grid\n",
    "param_grid_svm = {\n",
    "    'C': [0.1, 1, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100],\n",
    "    'kernel': ['linear', 'rbf']\n",
    "}\n",
    "\n",
    "# Model\n",
    "svm_model = SVC()\n",
    "grid_search_svm = GridSearchCV(svm_model, param_grid_svm, cv=5, scoring='accuracy')\n",
    "grid_search_svm.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters\n",
    "print(\"Best parameters for SVM:\", grid_search_svm.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression_optimized = LogisticRegression(**grid_search_lr.best_params_)\n",
    "logistic_regression_optimized.fit(X_train, y_train)\n",
    "svm_optimized = SVC(**grid_search_svm.best_params_)\n",
    "svm_optimized.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Evaluate each model\n",
    "models = [logistic_regression_optimized, svm_optimized]\n",
    "model_names = [\"Logistic Regression\", \"SVM\"]\n",
    "\n",
    "for model, name in zip(models, model_names):\n",
    "    predictions = model.predict(X_val)\n",
    "    print(f\"Model: {name}\")\n",
    "    print(classification_report(y_val, predictions))\n",
    "    print(\"Accuracy:\", accuracy_score(y_val, predictions))\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unlabeded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabelled_test_data = pd.read_csv('Data/unlabelled_test_data.csv')\n",
    "unlabelled_test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CamembertTokenizer, CamembertModel\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the tokenizer and model\n",
    "tokenizer = CamembertTokenizer.from_pretrained(\"camembert-base\")\n",
    "model = CamembertModel.from_pretrained(\"camembert-base\")\n",
    "\n",
    "# Function to encode sentences in batches\n",
    "def encode_sentences(sentences, batch_size=64):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    batched_embeddings = []\n",
    "\n",
    "    for i in range(0, len(sentences), batch_size):\n",
    "        batch_sentences = sentences[i:i + batch_size]\n",
    "        inputs = tokenizer(batch_sentences, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state[:, 0, :].numpy()\n",
    "        batched_embeddings.append(embeddings)\n",
    "\n",
    "    return np.vstack(batched_embeddings)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the cleaning function to the dataset\n",
    "unlabelled_test_data['cleaned_sentence'] = unlabelled_test_data['sentence'].apply(clean_text)\n",
    "\n",
    "# Feature Engineering\n",
    "# Adding sentence length and word count\n",
    "unlabelled_test_data['sentence_length'] = unlabelled_test_data['cleaned_sentence'].apply(len)\n",
    "unlabelled_test_data['word_count'] = unlabelled_test_data['cleaned_sentence'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Adding POS tagging\n",
    "unlabelled_test_data['pos_tags'] = unlabelled_test_data['cleaned_sentence'].apply(pos_tagging)\n",
    "\n",
    "#encoding data\n",
    "embeddings = encode_sentences(unlabelled_test_data['sentence'].tolist())\n",
    "\n",
    "\n",
    "# Embeddings are now stored in the 'embeddings' variable\n",
    "embeddings.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained CamemBERT predicitons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import CamembertTokenizer, CamembertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Load the fine-tuned model\n",
    "model = CamembertForSequenceClassification.from_pretrained(\"./my_finetuned_camembert\")\n",
    "tokenizer = CamembertTokenizer.from_pretrained(\"camembert-base\")\n",
    "\n",
    "# Load the unlabelled test data\n",
    "unlabelled_test_data = pd.read_csv('Data/unlabelled_test_data.csv')\n",
    "\n",
    "# Applying the cleaning function to the dataset (make sure this function is defined in your environment)\n",
    "unlabelled_test_data['cleaned_sentence'] = unlabelled_test_data['sentence'].apply(clean_text)\n",
    "\n",
    "# Function to encode and predict labels for sentences\n",
    "def predict_labels(sentences, model, tokenizer, batch_size=64):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    predictions = []\n",
    "\n",
    "    for i in range(0, len(sentences), batch_size):\n",
    "        batch_sentences = sentences[i:i + batch_size]\n",
    "        inputs = tokenizer(batch_sentences, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        batch_predictions = torch.argmax(logits, dim=1).numpy()\n",
    "        predictions.extend(batch_predictions)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "# Predict labels for the unlabelled test data\n",
    "predicted_labels = predict_labels(unlabelled_test_data['cleaned_sentence'].tolist(), model, tokenizer)\n",
    "\n",
    "# Convert numerical labels back to original class names\n",
    "predicted_classes = label_encoder.inverse_transform(predicted_labels)\n",
    "\n",
    "# Adding predictions to the DataFrame\n",
    "unlabelled_test_data['difficulty'] = predicted_classes\n",
    "\n",
    "# Displaying the predictions\n",
    "unlabelled_test_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'id' is the column name for the identifier in your DataFrame\n",
    "# and 'predicted_difficulty' is the column with the predicted difficulty levels\n",
    "\n",
    "# Saving the specified columns to a CSV file\n",
    "unlabelled_test_data[['id', 'difficulty']].to_csv('Data/CamemBERT_pretrained_submission.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Convert the series of arrays into a 2D NumPy array\n",
    "#unlabeled_test_features_array = np.array([feature for feature in unlabeled_test_features])\n",
    "\n",
    "# Now use the SVM model to make predictions\n",
    "# Ensure unlabeled_test_features_array is in the correct format expected by the SVM model\n",
    "predictions = svm_optimized.predict(embeddings)\n",
    "\n",
    "# Convert Predictions back to Difficulty Labels\n",
    "predicted_difficulties = label_encoder.inverse_transform(predictions)\n",
    "\n",
    "# Step 5: Output the Predictions\n",
    "unlabelled_test_data['difficulty'] = predicted_difficulties\n",
    "unlabelled_test_data[['id', 'difficulty']].to_csv('Data/CamemBERT_submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
